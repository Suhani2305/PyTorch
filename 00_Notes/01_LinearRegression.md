## Linear Regression - THE FOUNDATION

### ğŸ¯ What is Linear Regression?

**Simple Definition:** A method to predict **numerical values** using a straight-line relationship between inputs and outputs.

**Real-World Examples:**

| Problem | Input Features | Output (What we predict) |
|---------|---------------|-------------------------|
| ğŸ  **House Pricing** | Area (sq ft), Age (years), Bedrooms | Price ($) |
| ğŸ“ˆ **Stock Prediction** | Previous prices, Volume, Market indicators | Tomorrow's price |
| ğŸ¥ **Hospital Stay** | Age, Disease severity, Treatment type | Days in hospital |
| ğŸ›’ **Retail Sales** | Season, Promotions, Weather | Number of items sold |
| ğŸš— **Car Price** | Mileage, Year, Brand, Condition | Resale value |

---

### ğŸ“– Machine Learning Terminology - MUST KNOW!

#### 1. **Dataset Components**

```
DATASET (Complete collection of data)
â”‚
â”œâ”€ Training Set (used to train the model)
â”‚  â”œâ”€ Example/Sample/Instance/Data Point (one row)
â”‚  â”‚  â”œâ”€ Features/Covariates (input variables: xâ‚, xâ‚‚, ..., xâ‚)
â”‚  â”‚  â””â”€ Label/Target (what we want to predict: y)
â”‚  â”‚
â”‚  â””â”€ Example: {area=1500 sq ft, age=10 years} â†’ price=$300,000
â”‚
â””â”€ Test Set (used to evaluate the model)
```

#### 2. **Detailed Example: House Price Prediction**

**Scenario:** We want to predict house prices

```
Raw Data Table:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Area    â”‚ Age      â”‚ Price     â”‚
â”‚ (sq ft) â”‚ (years)  â”‚ ($)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1500    â”‚ 10       â”‚ 300,000   â”‚ â† One example/sample
â”‚ 2000    â”‚ 5        â”‚ 450,000   â”‚ â† Another example
â”‚ 1200    â”‚ 15       â”‚ 250,000   â”‚
â”‚  ...    â”‚ ...      â”‚ ...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†‘         â†‘           â†‘
 Feature 1  Feature 2   Label (y)
   (xâ‚)       (xâ‚‚)
```

**Terminology mapping:**
- **n** = number of examples (3 shown above, could be 1000s)
- **d** = number of features (2: area and age)
- **Feature dimensionality** = d = 2

 

#### **Basic Intuition:**

Think of it like a recipe:
```
Final Price = (Area Ã— price_per_sqft) + (Age Ã— age_penalty) + base_price
```

#### **Mathematical Form:**

**Long form (explicit):**
```
price = w_area Ã— area + w_age Ã— age + b
```

Where:
- **w_area** = weight for area (e.g., $200/sq ft)
- **w_age** = weight for age (e.g., -$5000/year penalty)
- **b** = bias/intercept (base price when area=0, age=0)

**Example Calculation:**
```
If: w_area = 200, w_age = -5000, b = 100,000
And: area = 1500, age = 10

Then:
price = 200Ã—1500 + (-5000)Ã—10 + 100,000
      = 300,000 - 50,000 + 100,000
      = 350,000
```

#### **Compact Vector Form:**

```
Å· = w^T x + b
```

**Breaking it down:**

```
w = [wâ‚]      x = [xâ‚]      w^T x = wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + ... + wâ‚Ã—xâ‚
    [wâ‚‚]          [xâ‚‚]
    [...]          [...]
    [wâ‚]          [xâ‚]
    
Weight vector  Feature vector  Dot product
```

**The "hat" symbol (Å·):**
- Å· = "y-hat" = **predicted** value
- y = actual/true value
- Always distinguish: Å· (prediction) vs y (reality)

#### **Matrix Form for Multiple Examples:**

```
Design Matrix X (n Ã— d):

       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ xâ‚â½Â¹â¾  xâ‚‚â½Â¹â¾ ... xâ‚â½Â¹â¾â”‚ â† Example 1
       â”‚ xâ‚â½Â²â¾  xâ‚‚â½Â²â¾ ... xâ‚â½Â²â¾â”‚ â† Example 2
   X = â”‚  ...    ...  ...  ... â”‚
       â”‚ xâ‚â½â¿â¾  xâ‚‚â½â¿â¾ ... xâ‚â½â¿â¾â”‚ â† Example n
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       n rows (examples) Ã— d columns (features)

Predictions for all examples:
Å· = Xw + b
```

---

###  Loss Function - Measuring Our Mistakes

#### **Why Do We Need a Loss Function?**

**Problem:** Our model makes predictions, but how do we know if they're good or bad?

**Solution:** Define a **loss function** that quantifies the error

#### **Squared Error Loss - The Most Common Choice**

**For a Single Example:**

```
l^(i)(w, b) = 1/2 (Å·^(i) - y^(i))Â²

Example:
True price (y): $300,000
Predicted (Å·): $320,000
Error: $20,000

Loss = 1/2 Ã— (20,000)Â²
     = 1/2 Ã— 400,000,000
     = 200,000,000
```

**Why square the error?**
1. âœ… Makes all errors positive (no cancellation)
2. âœ… Penalizes large errors more heavily
3. âœ… Mathematically convenient (smooth, differentiable)
4. âœ… Has nice statistical properties

**Why the 1/2?**
- Makes calculus cleaner
- When we differentiate: d/dx (1/2 xÂ²) = x (the 1/2 and 2 cancel!)

**Visual Understanding:**

```
Squared Error Growth:

Error    â†’  $1K    $10K   $100K
Loss     â†’  $0.5M  $50M   $5,000M
             â†‘       â†‘       â†‘
         Small   Medium   HUGE penalty
```

#### **Total Loss Over Entire Dataset:**

```
L(w, b) = 1/n Î£áµ¢â‚Œâ‚â¿ l^(i)(w, b)
        = 1/n Î£áµ¢â‚Œâ‚â¿ 1/2(Å·^(i) - y^(i))Â²
        = 1/n Î£áµ¢â‚Œâ‚â¿ 1/2(w^T x^(i) + b - y^(i))Â²
```

**Mean Squared Error (MSE):**
- We average (Ã·n) to make loss independent of dataset size
- Loss of 100 on 10 examples = same average as loss of 1000 on 100 examples

#### **Goal of Training:**

```
Find: w*, b* = argmin L(w, b)
              w,b

In words: Find the weights and bias that minimize average loss
```

---

### ğŸ”§ Optimization - Finding the Best Parameters

#### **Analytic Solution (Closed Form)**

**The Math:**

For linear regression, we can solve directly:
```
w* = (X^T X)^(-1) X^T y
```

**Requirements:**
- X^T X must be **invertible**
- Features must be **linearly independent**
- Works ONLY for linear regression

**Why we don't always use it:**
1. âŒ Matrix inversion is expensive for large d (O(dÂ³))
2. âŒ Requires full dataset in memory
3. âŒ Only works for linear models
4. âŒ Most deep learning problems don't have closed form solutions

**When to use:**
âœ… Small datasets (< 10,000 examples)
âœ… Few features (< 100)
âœ… Simple linear regression
âœ… Exact solution needed

---

#### **Gradient Descent - The Iterative Approach**

**Core Idea:** 
- Start with random parameters
- Repeatedly take small steps in the direction that reduces loss
- Like hiking down a mountain blindfolded - always step downhill

**Full Batch Gradient Descent:**

```python
# Pseudocode
w, b = initialize_randomly()

for iteration in range(num_iterations):
    # Compute gradient using ALL examples
    grad_w = (1/n) Î£áµ¢â‚Œâ‚â¿ âˆ‚l^(i)/âˆ‚w
    grad_b = (1/n) Î£áµ¢â‚Œâ‚â¿ âˆ‚l^(i)/âˆ‚b
    
    # Update parameters
    w = w - learning_rate Ã— grad_w
    b = b - learning_rate Ã— grad_b
```

**Problems with Full Batch:**
- âŒ Must process ENTIRE dataset for one update
- âŒ Very slow for large datasets (millions of examples)
- âŒ Redundant computation if data has duplicates
- âŒ Memory intensive

---

#### **Stochastic Gradient Descent (SGD)**

**Extreme opposite:** Use only ONE example at a time

```python
# Pseudocode
w, b = initialize_randomly()

for iteration in range(num_iterations):
    # Pick ONE random example
    i = random_index()
    
    # Compute gradient on this single example
    grad_w = âˆ‚l^(i)/âˆ‚w
    grad_b = âˆ‚l^(i)/âˆ‚b
    
    # Update
    w = w - learning_rate Ã— grad_w
    b = b - learning_rate Ã— grad_b
```

**Problems with Pure SGD:**
- âŒ Very noisy updates (high variance)
- âŒ Inefficient use of modern hardware (GPUs/CPUs)
- âŒ Doesn't work well with batch normalization
- âŒ Can be unstable

---

#### **Minibatch SGD - THE GOLDILOCKS SOLUTION** â­

**Best of both worlds:** Use small batches of examples

```python
# Pseudocode
w, b = initialize_randomly()
batch_size = 32  # Typical: 32, 64, 128, 256

for epoch in range(num_epochs):
    shuffle(dataset)  # Important!
    
    for batch in get_batches(dataset, batch_size):
        # Compute gradient on minibatch
        grad_w = (1/batch_size) Î£áµ¢âˆˆbatch âˆ‚l^(i)/âˆ‚w
        grad_b = (1/batch_size) Î£áµ¢âˆˆbatch âˆ‚l^(i)/âˆ‚b
        
        # Update
        w = w - learning_rate Ã— grad_w
        b = b - learning_rate Ã— grad_b
```

**Why Minibatch is Best:**

| Aspect | Full Batch | Minibatch | Single Example |
|--------|-----------|-----------|----------------|
| **Speed** | âŒ Slowest | âœ… Fast | âš ï¸ Medium |
| **Memory** | âŒ High | âœ… Moderate | âœ… Low |
| **Hardware efficiency** | âš ï¸ OK | âœ… Excellent | âŒ Poor |
| **Gradient accuracy** | âœ… Perfect | âœ… Good | âŒ Noisy |
| **Convergence** | âš ï¸ Smooth | âœ… Stable | âŒ Unstable |

**Choosing Batch Size:**

```
Factors to consider:

1. GPU Memory:
   - Larger batch = more memory
   - Typical: 32-256 for standard GPUs

2. Model Architecture:
   - Batch normalization needs batch_size > 1
   - Prefer multiples of 8, 16, 32 (hardware optimization)

3. Dataset Size:
   - Small dataset: smaller batches (32)
   - Large dataset: larger batches (256)

4. Learning Dynamics:
   - Smaller batch = more noise = better exploration
   - Larger batch = more stable = faster convergence

Common choices:
- Small models/datasets: 32
- Medium: 64-128
- Large models/datasets: 256
```

---

#### **Update Rules - The Math in Detail**

**For Squared Loss:**

```
Gradient of loss w.r.t. weights:
âˆ‚L/âˆ‚w = (1/|B|) Î£áµ¢âˆˆB x^(i) (w^T x^(i) + b - y^(i))

Gradient w.r.t. bias:
âˆ‚L/âˆ‚b = (1/|B|) Î£áµ¢âˆˆB (w^T x^(i) + b - y^(i))
```

**Update Step:**

```
w â† w - Î· Ã— (1/|B|) Î£áµ¢âˆˆB x^(i) (w^T x^(i) + b - y^(i))
b â† b - Î· Ã— (1/|B|) Î£áµ¢âˆˆB (w^T x^(i) + b - y^(i))
```

Where:
- **Î· (eta)** = learning rate
- **|B|** = minibatch size
- **Î£áµ¢âˆˆB** = sum over examples in the minibatch

---

### ğŸ›ï¸ Hyperparameters - What YOU Must Choose

**Definition:** Parameters that are NOT learned by the model, but set by YOU

#### **Key Hyperparameters:**

1. **Learning Rate (Î·)**

```
Too Small (Î· = 0.0001):
- Training is VERY slow
- Might not converge in reasonable time
- Safe but inefficient

Good (Î· = 0.01):
- Steady progress
- Stable convergence
- Sweet spot!

Too Large (Î· = 1.0):
- Training explodes
- Loss goes to infinity
- Model diverges
```

**Visual:**
```
Loss landscape:

        â•±\      Î· too large
       â•±  \     (overshoots)
      â•±    \    
â”€â”€â”€â”€â”€â•±      \â”€â”€â”€â”€â”€â”€â”€
    /        \
   /    â€¢     \    Î· good (reaches minimum)
  /   â†™ â†˜     \
 /  â†™     â†˜    \
/__________\____\__ 
            â†‘
         minimum

  â€¢          Î· too small (gets stuck)
```

2. **Batch Size (|B|)**
- Small (16-32): More noise, better exploration, slower
- Large (128-256): Less noise, faster, needs more memory

3. **Number of Epochs**
- Too few: Underfitting
- Too many: Overfitting
- Monitor validation loss to decide

4. **Initialization Scale (Ïƒ)**
- Weights initialized from N(0, ÏƒÂ²)
- Too small: slow learning
- Too large: instability
- Typical: Ïƒ = 0.01

---

### ğŸ“Š Training Process - Step by Step

**Complete Algorithm:**

```
INITIALIZATION:
â”œâ”€ w âˆ¼ N(0, 0.01Â²)  (small random weights)
â”œâ”€ b = 0             (zero bias)
â”œâ”€ Î· = 0.03          (learning rate)
â””â”€ batch_size = 32

FOR EACH EPOCH (epoch = 1, 2, ..., max_epochs):
â”‚
â”œâ”€ Shuffle training data (very important!)
â”‚
â”œâ”€ FOR EACH MINIBATCH:
â”‚  â”‚
â”‚  â”œâ”€ 1. FORWARD PASS:
â”‚  â”‚    â”œâ”€ Get batch: (X_batch, y_batch)
â”‚  â”‚    â””â”€ Compute: Å·_batch = X_batch @ w + b
â”‚  â”‚
â”‚  â”œâ”€ 2. COMPUTE LOSS:
â”‚  â”‚    â””â”€ L = mean((Å·_batch - y_batch)Â²)
â”‚  â”‚
â”‚  â”œâ”€ 3. BACKWARD PASS:
â”‚  â”‚    â”œâ”€ Compute: âˆ‚L/âˆ‚w, âˆ‚L/âˆ‚b
â”‚  â”‚    â””â”€ (Automatic differentiation does this!)
â”‚  â”‚
â”‚  â””â”€ 4. UPDATE PARAMETERS:
â”‚       â”œâ”€ w â† w - Î· Ã— âˆ‚L/âˆ‚w
â”‚       â””â”€ b â† b - Î· Ã— âˆ‚L/âˆ‚b
â”‚
â””â”€ VALIDATE (optional but recommended):
   â”œâ”€ Compute loss on validation set
   â”œâ”€ Track validation error
   â””â”€ Check for overfitting
```

---

### ğŸ§ª Mathematical Connection: Maximum Likelihood Estimation

**The Probabilistic View:**

Instead of "minimize squared loss", we can think:
"What's the most likely model given the data?"

#### **Assumptions:**

```
1. True relationship is linear with noise:
   y = w^T x + b + Îµ
   
2. Noise is Gaussian:
   Îµ âˆ¼ N(0, ÏƒÂ²)
   
3. Therefore:
   y | x âˆ¼ N(w^T x + b, ÏƒÂ²)
```

**Probability of observing y given x:**

```
p(y | x) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-(y - w^T x - b)Â² / (2ÏƒÂ²))
```

#### **Maximum Likelihood:**

```
Goal: Find w, b that maximize probability of observing our data

Likelihood = Product of probabilities:
L(w, b) = âˆáµ¢â‚Œâ‚â¿ p(y^(i) | x^(i))

Take log (easier to work with):
log L(w, b) = Î£áµ¢â‚Œâ‚â¿ log p(y^(i) | x^(i))
            = Î£áµ¢â‚Œâ‚â¿ [log(1/âˆš(2Ï€ÏƒÂ²)) - (y^(i) - w^T x^(i) - b)Â²/(2ÏƒÂ²)]
            
Maximize log L âŸº Minimize Î£áµ¢â‚Œâ‚â¿ (y^(i) - w^T x^(i) - b)Â²
```

**KEY INSIGHT:** 
```
Minimizing squared loss = Maximum likelihood estimation
(when we assume Gaussian noise)
```

---

### ğŸ§  Linear Regression as a Neural Network

**Network Diagram:**

```
INPUT LAYER          OUTPUT LAYER
    
    xâ‚ â”€â”€â”€â”€â”€â”€wâ‚â”€â”€â”€â”€â”€â”
                     â”‚
    xâ‚‚ â”€â”€â”€â”€â”€â”€wâ‚‚â”€â”€â”€â”€â”€â”¤
                     â”œâ”€â”€â†’ (+b) â”€â”€â†’ Å·
    xâ‚ƒ â”€â”€â”€â”€â”€â”€wâ‚ƒâ”€â”€â”€â”€â”€â”¤
                     â”‚
    ...              â”‚
                     â”‚
    xâ‚ â”€â”€â”€â”€â”€â”€wâ‚â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  All inputs connected to output     â”‚
â”‚  No hidden layers                   â”‚
â”‚  Single neuron (output)             â”‚
â”‚  Fully connected                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components:**

1. **Input Layer:** 
   - d neurons (one per feature)
   - No computation, just pass values

2. **Connections:**
   - Each input â†’ output has a weight
   - Total weights = d

3. **Output Layer:**
   - 1 neuron (for regression)
   - Computes: Î£(wáµ¢xáµ¢) + b

4. **Activation Function:**
   - None! (or identity: f(x) = x)
   - This is what makes it LINEAR

---

### ğŸ§¬ Biological Inspiration

**Real Neuron Structure:**

```
BIOLOGICAL NEURON:

Dendrites â”€â”€â†’ Cell Body â”€â”€â†’ Axon â”€â”€â†’ Axon Terminals
   (input)    (processing)  (output)   (to other neurons)
   
Information flow:
1. Dendrites receive signals (xáµ¢)
2. Signals weighted by synapse strength (wáµ¢)
3. Cell body aggregates: Î£ wáµ¢xáµ¢
4. Activation function Ïƒ(Â·) fires if threshold exceeded
5. Signal travels down axon
6. Axon terminals pass to next neurons
```

**Artificial Neuron (Our Model):**

```
xâ‚ â”€â”€â”
xâ‚‚ â”€â”€â”¤
xâ‚ƒ â”€â”€â”¼â”€â”€â†’ [Î£ wáµ¢xáµ¢ + b] â”€â”€â†’ Å·
... â”€â”¤
xâ‚ â”€â”€â”˜

Same concept:
- Multiple inputs (dendrites)
- Weighted sum (cell body)
- Output (axon)
```

**Important Note:**
Modern deep learning is INSPIRED by neuroscience, but:
- âŒ Not trying to replicate brain exactly
- âœ… Using math/engineering principles
- âœ… Drawing from many fields: stats, optimization, CS, etc.

Like airplanes vs birds:
- Inspired by bird flight
- But don't flap wings!
- Use aerodynamics and engineering

---

### ğŸ“ Summary of Chapter 3.1

**What We Learned:**

1. âœ… Linear regression predicts numerical values
2. âœ… Model: Å· = w^T x + b
3. âœ… Loss: Mean squared error
4. âœ… Training: Minibatch SGD
5. âœ… Connection to statistics: MLE with Gaussian noise
6. âœ… Simplest neural network (1 layer, no hidden units)

**Key Equations:**

```
Model:        Å· = w^T x + b
Loss:         L = (1/n) Î£ (Å·^(i) - y^(i))Â²
Update:       w â† w - Î· âˆ‚L/âˆ‚w
              b â† b - Î· âˆ‚L/âˆ‚b
```

---

## ğŸ—ï¸ Chapter 3.2: Object-Oriented Design for Implementation

### ğŸ¯ Why Object-Oriented Design?

**Problem:** Deep learning code can get messy fast!
- Models have many components
- Data preprocessing is complex
- Training loops are repetitive
- Hard to reuse code

**Solution:** Organize code into reusable classes!

---

### ğŸ§© The Three Core Classes

#### **Architecture Overview:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TRAINER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  fit(model, data):                             â”‚    â”‚
â”‚  â”‚    for epoch in epochs:                        â”‚    â”‚
â”‚  â”‚      train_step() â”€â”€â”€â†’ MODEL.training_step()   â”‚    â”‚
â”‚  â”‚      valid_step() â”€â”€â”€â†’ MODEL.validation_step() â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚
           â†“                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       MODULE         â”‚   â”‚    DATAMODULE        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ forward()      â”‚  â”‚   â”‚  â”‚ train_loader() â”‚  â”‚
â”‚  â”‚ loss()         â”‚  â”‚   â”‚  â”‚ val_loader()   â”‚  â”‚
â”‚  â”‚ training_step()â”‚  â”‚   â”‚  â”‚ test_loader()  â”‚  â”‚
â”‚  â”‚ configure_opt()â”‚  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“¦ Class 1: Module (The Model)

**Purpose:** Contains everything about the MODEL itself

```python
class Module(nn.Module):
    """
    Base class for all models
    
    Responsibilities:
    1. Store learnable parameters (weights, biases)
    2. Define forward pass (how to compute predictions)
    3. Define loss function
    4. Define training step (what happens per batch)
    5. Configure optimizer
    """
    
    def __init__(self):
        """Initialize model architecture and parameters"""
        super().__init__()
        self.board = ProgressBoard()  # For visualization
        
    def forward(self, X):
        """
        Compute predictions
        
        Args:
            X: Input features (batch_size Ã— num_features)
        
        Returns:
            predictions (batch_size Ã— output_dim)
        """
        raise NotImplementedError
        
    def loss(self, y_hat, y):
        """
        Compute loss between predictions and targets
        
        Args:
            y_hat: Predictions
            y: True labels
            
        Returns:
            scalar loss value
        """
        raise NotImplementedError
        
    def training_step(self, batch):
        """
        What happens during one training iteration
        
        Args:
            batch: (X, y) tuple from data loader
            
        Returns:
            loss value
        """
        # Unpack batch
        X, y = batch[:-1], batch[-1]
        
        # Forward pass
        y_hat = self(X)
        
        # Compute loss
        l = self.loss(y_hat, y)
        
        # Log for visualization
        self.plot('loss', l, train=True)
        
        return l
        
    def validation_step(self, batch):
        """What happens during validation"""
        X, y = batch[:-1], batch[-1]
        y_hat = self(X)
        l = self.loss(y_hat, y)
        self.plot('loss', l, train=False)
        
    def configure_optimizers(self):
        """Return optimizer(s) for training"""
        raise NotImplementedError
        
    def plot(self, key, value, train):
        """Helper for logging metrics"""
        # Implementation in full code
        pass
```

**Key Methods Explained:**

1. **`__init__`**: Constructor
   - Sets up model architecture
   - Initializes parameters
   - Creates visualization board

2. **`forward(X)`**: The prediction function
   - Takes inputs X
   - Returns predictions Å·
   - Called automatically when you do `model(X)`

3. **`loss(y_hat, y)`**: Loss calculation
   - Compares predictions vs truth
   - Returns a scalar (single number)

4. **`training_step(batch)`**: One training iteration
   - Gets a batch of data
   - Computes predictions
   - Computes loss
   - Returns loss (for backward pass)

5. **`validation_step(batch)`**: One validation iteration
   - Same as training but no gradient updates
   - Used to monitor overfitting

6. **`configure_optimizers()`**: Setup optimizer
   - Returns optimizer object (SGD, Adam, etc.)
   - Specifies learning rate, momentum, etc.

---

### ğŸ“Š Class 2: DataModule (The Data)

**Purpose:** Contains everything about DATA handling

```python
class DataModule:
    """
    Base class for data
    
    Responsibilities:
    1. Download/prepare data
    2. Preprocess data
    3. Create train/val/test splits
    4. Provide data loaders
    """
    
    def __init__(self, root='../data', num_workers=4):
        """
        Args:
            root: Where to store/load data
            num_workers: Parallel data loading threads
        """
        self.save_hyperparameters()
        
    def get_dataloader(self, train):
        """
        Create data loader
        
        Args:
            train: If True, return training loader
                   If False, return validation loader
        """
        raise NotImplementedError
        
    def train_dataloader(self):
        """Return training data loader"""
        return self.get_dataloader(train=True)
        
    def val_dataloader(self):
        """Return validation data loader"""
        return self.get_dataloader(train=False)
        
    def test_dataloader(self):
        """Return test data loader"""
        # Optional: for final evaluation
        pass
```

**Data Loader Concept:**

```python
# A data loader is a GENERATOR that yields batches

dataloader = data.train_dataloader()

for batch in dataloader:  # Iterates through dataset
    X, y = batch
    # X shape: (batch_size, num_features)
    # y shape: (batch_size, 1)
    
    # Do something with batch
    predictions = model(X)
    loss = loss_fn(predictions, y)
```

**Why Use Data Loaders?**

```
WITHOUT Data Loader:
â”œâ”€ Must manually batch data
â”œâ”€ Must manually shuffle
â”œâ”€ Must handle last batch (might be smaller)
â”œâ”€ Hard to parallelize
â””â”€ Lots of boilerplate code

WITH Data Loader:
â”œâ”€ âœ… Automatic batching
â”œâ”€ âœ… Automatic shuffling
â”œâ”€ âœ… Handles edge cases
â”œâ”€ âœ… Parallel data loading
â””â”€ âœ… Clean, simple code
```

---

### ğŸ® Class 3: Trainer (The Training Loop)

**Purpose:** Orchestrates the training process

```python
class Trainer:
    """
    Base class for training models
    
    Responsibilities:
    1. Run training loop
    2. Handle epochs and batches
    3. Call model's training/validation steps
    4. Track progress
    5. Handle multi-GPU training (advanced)
    """
    
    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
        """
        Args:
            max_epochs: How many times to iterate through data
            num_gpus: Number of GPUs to use
            gradient_clip_val: Clip gradients (prevent explosion)
        """
        self.save_hyperparameters()
        
    def prepare_data(self, data):
        """Get data loaders from DataModule"""
        self.train_dataloader = data.train_dataloader()
        self.val_dataloader = data.val_dataloader()
        self.num_train_batches = len(self.train_dataloader)
        self.num_val_batches = len(self.val_dataloader)
        
    def prepare_model(self, model):
        """Setup model for training"""
        model.trainer = self  # Give model access to trainer
        model.board.xlim = [0, self.max_epochs]  # Set plot limits
        self.model = model
        
    def fit(self, model, data):
        """
        Main training loop
        
        Args:
            model: Module instance
            data: DataModule instance
        """
        # Setup
        self.prepare_data(data)
        self.prepare_model(model)
        self.optim = model.configure_optimizers()
        
        # Training loop
        for self.epoch in range(self.max_epochs):
            self.fit_epoch()
            
    def fit_epoch(self):
        """Train for one epoch"""
        # Will be implemented in detail later
        raise NotImplementedError
```

**Complete Training Flow:**

```
trainer.fit(model, data)
    â”‚
    â”œâ”€â†’ prepare_data(data)
    â”‚     â””â”€ Get train/val loaders
    â”‚
    â”œâ”€â†’ prepare_model(model)
    â”‚     â””â”€ Link model â†” trainer
    â”‚
    â”œâ”€â†’ Get optimizer from model
    â”‚
    â””â”€â†’ FOR epoch in range(max_epochs):
          â”‚
          â”œâ”€â†’ fit_epoch()
          â”‚     â”‚
          â”‚     â”œâ”€ TRAINING PHASE:
          â”‚     â”‚   FOR batch in train_dataloader:
          â”‚     â”‚     â”œâ”€ loss = model.training_step(batch)
          â”‚     â”‚     â”œâ”€ optimizer.zero_grad()
          â”‚     â”‚     â”œâ”€ loss.backward()
          â”‚     â”‚     â””â”€ optimizer.step()
          â”‚     â”‚
          â”‚     â””â”€ VALIDATION PHASE:
          â”‚         FOR batch in val_dataloader:
          â”‚           â””â”€ model.validation_step(batch)
          â”‚
          â””â”€ Plot/log results
```

---

### ğŸ› ï¸ Utility Functions

#### **1. `@add_to_class` Decorator**

**Problem:** In Jupyter notebooks, we want to split class definitions across cells

**Solution:** Add methods AFTER class is created

```python
def add_to_class(Class):
    """
    Decorator to register function as method in existing class
    
    Usage:
        class A:
            def __init__(self):
                self.x = 1
        
        @add_to_class(A)
        def new_method(self):
            return self.x + 1
        
        a = A()
        a.new_method()  # Returns 2
    """
    def wrapper(obj):
        setattr(Class, obj.__name__, obj)
        return obj
    return wrapper
```

**Detailed Example:**

```python
# Step 1: Define basic class
class Calculator:
    def __init__(self, value):
        self.value = value

# Step 2: Create instance
calc = Calculator(10)

# Step 3: Add method AFTER creation
@add_to_class(Calculator)
def add(self, x):
    return self.value + x

@add_to_class(Calculator)
def multiply(self, x):
    return self.value * x

# Step 4: Use new methods
print(calc.add(5))       # 15
print(calc.multiply(3))  # 30

# Even new instances have these methods!
calc2 = Calculator(20)
print(calc2.add(5))      # 25
```

**Why This is Useful:**

```
Jupyter Notebook Flow:

Cell 1:
  class Model:
      def __init__(self): ...

Cell 2 (explanation of forward pass):
  @add_to_class(Model)
  def forward(self, X): ...

Cell 3 (explanation of loss):
  @add_to_class(Model)
  def loss(self, y_hat, y): ...

Benefits:
â”œâ”€ âœ… Each cell focuses on one concept
â”œâ”€ âœ… Can add explanatory text between methods
â”œâ”€ âœ… More readable notebook
â””â”€ âœ… Methods still part of class
```

---

#### **2. `HyperParameters` Class**

**Problem:** Lots of boilerplate saving constructor arguments

**Bad Way (Manual):**

```python
class Model:
    def __init__(self, lr, batch_size, num_layers, hidden_dim):
        self.lr = lr                    # Repetitive!
        self.batch_size = batch_size    # Annoying!
        self.num_layers = num_layers    # Error-prone!
        self.hidden_dim = hidden_dim    # So much typing!
```

**Good Way (HyperParameters):**

```python
class Model(HyperParameters):
    def __init__(self, lr, batch_size, num_layers, hidden_dim):
        self.save_hyperparameters()  # That's it!
        # Now self.lr, self.batch_size, etc. are automatically saved
```

**Implementation:**

```python
class HyperParameters:
    """Automatically save constructor arguments as attributes"""
    
    def save_hyperparameters(self, ignore=[]):
        """
        Save all __init__ arguments as instance attributes
        
        Args:
            ignore: List of argument names to NOT save
        """
        import inspect
        
        # Get the calling function's frame
        frame = inspect.currentframe().f_back
        
        # Get argument names and values
        args = inspect.getargvalues(frame)
        
        # Save each argument as attribute
        for arg in args.locals:
            if arg != 'self' and arg not in ignore:
                setattr(self, arg, args.locals[arg])
```

**Detailed Example:**

```python
class MyModel(HyperParameters):
    def __init__(self, lr, batch_size, dropout, secret_key):
        # Save everything except secret_key
        self.save_hyperparameters(ignore=['secret_key'])
        
        print(f"lr: {self.lr}")              # âœ… Saved
        print(f"batch_size: {self.batch_size}")  # âœ… Saved
        print(f"dropout: {self.dropout}")    # âœ… Saved
        # print(f"secret_key: {self.secret_key}")  # âŒ Not saved!

model = MyModel(lr=0.01, batch_size=32, dropout=0.5, secret_key="xyz")
```

**Use Cases:**

```
When to use save_hyperparameters():
âœ… Model hyperparameters (lr, layers, etc.)
âœ… Data parameters (batch_size, shuffle, etc.)
âœ… Training config (epochs, patience, etc.)

When NOT to use:
âŒ Large objects (datasets, models)
âŒ Sensitive information (passwords, keys)
âŒ Temporary variables
```

---

#### **3. `ProgressBoard` Class**

**Purpose:** Visualize training progress in real-time

```python
class ProgressBoard(HyperParameters):
    """
    Plot metrics during training
    
    Features:
    - Real-time updates
    - Multiple curves (train/val loss, accuracy, etc.)
    - Smoothing for noisy metrics
    - Customizable appearance
    """
    
    def __init__(self, xlabel=None, ylabel=None, 
                 xlim=None, ylim=None,
                 xscale='linear', yscale='linear',
                 ls=['-', '--', '-.', ':'],
                 colors=['C0', 'C1', 'C2', 'C3'],
                 figsize=(3.5, 2.5), display=True):
        """
        Args:
            xlabel, ylabel: Axis labels
            xlim, ylim: Axis limits
            xscale, yscale: 'linear' or 'log'
            ls: Line styles for different curves
            colors: Colors for different curves
            figsize: Figure size
            display: Whether to show plot
        """
        self.save_hyperparameters()
        
    def draw(self, x, y, label, every_n=1):
        """
        Add point to plot
        
        Args:
            x: X-coordinate
            y: Y-coordinate  
            label: Curve name (e.g., 'train_loss')
            every_n: Plot every n-th point (for smoothing)
        """
        # Implementation details...
        pass
```

**Usage Example:**

```python
# Create board
board = ProgressBoard(xlabel='epoch', ylabel='loss')

# During training
for epoch in range(100):
    # Training
    for batch in train_loader:
        train_loss = compute_loss(batch)
        board.draw(epoch, train_loss, 'train_loss', every_n=5)
    
    # Validation
    val_loss = validate()
    board.draw(epoch, val_loss, 'val_loss', every_n=1)
```

**Result:**

```
Visualization:

Loss â†‘
    â”‚
 1.0â”‚ â—â—â—â—
    â”‚      â—â—â—
    â”‚         â—â—â—
 0.5â”‚            â—â—â—  train_loss (smooth)
    â”‚               â—â—â—
    â”‚                  â—â—
 0.0â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â—â”€  val_loss (jumpy)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ epoch
    0                    100
```

**Parameters Explained:**

```python
every_n=1:   Plot every point (jumpy for noisy data)
every_n=5:   Average every 5 points (smoother)
every_n=10:  Average every 10 points (very smooth)

Example:
Raw data:  [1.0, 0.9, 1.1, 0.8, 1.0, 0.7, ...]
every_n=1: [1.0, 0.9, 1.1, 0.8, 1.0, 0.7, ...]
every_n=3: [1.0, 0.93, 0.90, 0.83, ...]  (averaged)
```

---

### ğŸ”„ How Everything Fits Together

**Complete Example:**

```python
# ============================================
# 1. DEFINE MODEL
# ============================================
class LinearRegression(Module):
    def __init__(self, num_inputs, lr):
        super().__init__()
        self.save_hyperparameters()
        
        # Parameters
        self.w = torch.randn(num_inputs, 1, requires_grad=True)
        self.b = torch.zeros(1, requires_grad=True)
        
    def forward(self, X):
        return X @ self.w + self.b
        
    def loss(self, y_hat, y):
        return ((y_hat - y) ** 2).mean()
        
    def configure_optimizers(self):
        return SGD([self.w, self.b], lr=self.lr)

# ============================================
# 2. DEFINE DATA
# ============================================
class RegressionData(DataModule):
    def __init__(self, w_true, b_true, num_train, num_val, batch_size):
        super().__init__()
        self.save_hyperparameters()
        
        # Generate synthetic data
        n = num_train + num_val
        self.X = torch.randn(n, len(w_true))
        self.y = self.X @ w_true + b_true + torch.randn(n, 1) * 0.01
        
    def get_dataloader(self, train):
        if train:
            indices = range(self.num_train)
        else:
            indices = range(self.num_train, self.num_train + self.num_val)
            
        # Create data loader
        dataset = TensorDataset(self.X[indices], self.y[indices])
        return DataLoader(dataset, self.batch_size, shuffle=train)

# ============================================
# 3. CREATE INSTANCES
# ============================================
model = LinearRegression(num_inputs=2, lr=0.03)
data = RegressionData(
    w_true=torch.tensor([2.0, -3.4]), 
    b_true=4.2,
    num_train=1000,
    num_val=200,
    batch_size=32
)
trainer = Trainer(max_epochs=10)

# ============================================
# 4. TRAIN
# ============================================
trainer.fit(model, data)

# Behind the scenes:
# - Trainer gets data loaders from data
# - Trainer gets optimizer from model
# - For each epoch:
#     - For each batch in train_loader:
#         - Call model.training_step(batch)
#         - Compute gradients
#         - Update parameters
#     - For each batch in val_loader:
#         - Call model.validation_step(batch)
# - ProgressBoard updates plots
```

---

### ğŸ“š Benefits of This Design

**1. Separation of Concerns:**

```
MODULE       â†’ Model architecture, loss, forward pass
DATAMODULE   â†’ Data loading, preprocessing
TRAINER      â†’ Training loop, optimization

Each class has ONE job âœ…
```

**2. Reusability:**

```python
# Same Trainer works for ANY model!
trainer = Trainer(max_epochs=10)

# Linear regression
trainer.fit(linear_model, regression_data)

# Neural network (later)
trainer.fit(neural_net, image_data)

# Transformer (much later)
trainer.fit(transformer, text_data)
```

**3. Testability:**

```python
# Test model independently
model = MyModel()
y_hat = model(test_input)
assert y_hat.shape == expected_shape

# Test data independently  
data = MyData()
batch = next(iter(data.train_dataloader()))
assert len(batch) == 2  # (X, y)
```

**4. Flexibility:**

```python
# Easy to customize specific parts

class CustomModel(Module):
    def loss(self, y_hat, y):
        # Custom loss function!
        return my_special_loss(y_hat, y)

# Everything else stays the same
trainer.fit(CustomModel(), data)
```

---

### ğŸ’¡ Design Patterns Used

**1. Template Method Pattern:**

```python
# Base class defines structure
class Module:
    def training_step(self, batch):
        # Same for all models
        y_hat = self(batch[0])
        loss = self.loss(y_hat, batch[1])
        return loss
    
    def forward(self, X):
        # Subclass implements
        raise NotImplementedError
```

**2. Strategy Pattern:**

```python
# Different optimizers (strategies)
def configure_optimizers(self):
    if self.optimizer == 'sgd':
        return SGD(self.parameters(), lr=self.lr)
    elif self.optimizer == 'adam':
        return Adam(self.parameters(), lr=self.lr)
```

**3. Builder Pattern:**

```python
# Trainer builds the training process step by step
trainer.prepare_data(data)
trainer.prepare_model(model)
trainer.fit_epoch()
```

---

### ğŸ¯ Summary of Chapter 3.2

**What We Learned:**

1. âœ… How to organize ML code into classes
2. âœ… Module: Contains model logic
3. âœ… DataModule: Contains data logic
4. âœ… Trainer: Contains training logic
5. âœ… Utility decorators and classes
6. âœ… How to split class definitions across notebook cells
7. âœ… Benefits of separation of concerns

**Key Takeaways:**

```
Good Code Organization:
â”œâ”€ âœ… Separate concerns (model, data, training)
â”œâ”€ âœ… Reusable components
â”œâ”€ âœ… Easy to test
â”œâ”€ âœ… Easy to extend
â””â”€ âœ… Readable and maintainable

This design will be used throughout the entire book!
```

---

## ğŸ“Š Chapter 3.3: Synthetic Regression Data

### ğŸ¯ Why Synthetic Data?

**The Testing Problem:**

```
When building ML models, we need to know:
â”œâ”€ â“ Does our code work correctly?
â”œâ”€ â“ Is our math implementation right?
â”œâ”€ â“ Does the optimizer converge?
â””â”€ â“ Can we trust our results?

With REAL data:
â”œâ”€ âŒ Don't know the true parameters
â”œâ”€ âŒ Can't verify if we found the "right" answer
â””â”€ âŒ Hard to debug

With SYNTHETIC data:
â”œâ”€ âœ… We KNOW the true parameters
â”œâ”€ âœ… Can check if our model recovers them
â”œâ”€ âœ… Perfect for testing and debugging
â””â”€ âœ… Controlled experimentation
```

### ğŸ”¬ Generating Synthetic Data

#### **The Data Generation Process:**

**Step 1: Choose True Parameters**

```python
# These are the "ground truth" we want to recover
w_true = torch.tensor([2.0, -3.4])  # True weights
b_true = 4.2                         # True bias
```

**Step 2: Generate Random Features**

```python
n = 1000  # Number of examples
d = 2     # Number of features

# Features from standard normal distribution
X = torch.randn(n, d)

Shape:
X[0] = [xâ‚â½Â¹â¾, xâ‚‚â½Â¹â¾]  â† First example
X[1] = [xâ‚â½Â²â¾, xâ‚‚â½Â²â¾]  â† Second example
...
X[999] = [xâ‚â½Â¹â°â°â°â¾, xâ‚‚â½Â¹â°â°â°â¾]  â† Last example
```

**Step 3: Generate Labels with Noise**

```python
# True linear relationship
y_true = X @ w_true + b_true

# Add Gaussian noise
noise = torch.randn(n, 1) * 0.01  # Ïƒ = 0.01
y = y_true + noise

Mathematical formula:
yâ½â±â¾ = w_true^T xâ½â±â¾ + b_true + Îµâ½â±â¾
where Îµâ½â±â¾ âˆ¼ N(0, 0.01Â²)
```

**Visualizing One Example:**

```python
Example i=0:
X[0] = [2.2793, -0.2246]

Calculation:
y[0] = 2.0 Ã— 2.2793 + (-3.4) Ã— (-0.2246) + 4.2 + Îµ
     = 4.5586 + 0.7636 + 4.2 + 0.0056  (small noise)
     = 9.5278

Actually generated:
y[0] = 9.5014  (slightly different due to noise)
```

---

### ğŸ’¾ Complete Data Class Implementation

```python
class SyntheticRegressionData(DataModule):
    """
    Synthetic data for testing linear regression
    
    Generates data from:
    y = X @ w + b + noise
    """
    
    def __init__(self, w, b, noise=0.01, 
                 num_train=1000, num_val=1000, 
                 batch_size=32):
        """
        Args:
            w: True weight vector (d-dimensional)
            b: True bias (scalar)
            noise: Standard deviation of Gaussian noise
            num_train: Number of training examples
            num_val: Number of validation examples
            batch_size: Minibatch size
        """
        super().__init__()
        self.save_hyperparameters()
        
        # Total number of examples
        n = num_train + num_val
        
        # Generate features: X âˆ¼ N(0, I)
        self.X = torch.randn(n, len(w))
        
        # Generate noise: Îµ âˆ¼ N(0, noiseÂ²)
        noise_vec = torch.randn(n, 1) * noise
        
        # Generate labels: y = Xw + b + Îµ
        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise_vec
```

**Parameter Breakdown:**

```python
w = torch.tensor([2.0, -3.4])
â””â”€> d = 2 features
    First feature contributes:  +2.0 Ã— xâ‚
    Second feature contributes: -3.4 Ã— xâ‚‚

b = 4.2
â””â”€> Intercept: base value when all features = 0

noise = 0.01
â””â”€> Small random variation
    95% of noise is between Â±0.02
    Simulates measurement error

num_train = 1000
â””â”€> Examples for training the model

num_val = 1000
â””â”€> Examples for evaluating the model

batch_size = 32
â””â”€> Process 32 examples at a time
```

---

### ğŸ”„ Data Loading - Manual Implementation

**Goal:** Split data into batches and iterate through them

#### **Manual Iterator (From Scratch):**

```python
@add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    """
    Create a data loader
    
    Args:
        train: If True, shuffle and return train data
               If False, return validation data in order
    """
    if train:
        # Training indices
        indices = list(range(0, self.num_train))
        # Shuffle for better training
        random.shuffle(indices)
    else:
        # Validation indices
        indices = list(range(self.num_train, 
                           self.num_train + self.num_val))
    
    # Yield batches
    for i in range(0, len(indices), self.batch_size):
        # Get batch indices
        batch_indices = torch.tensor(
            indices[i : i + self.batch_size]
        )
        
        # Yield corresponding data
        yield self.X[batch_indices], self.y[batch_indices]
```

**How It Works:**

```
Training Mode (train=True):
â”œâ”€ indices = [0, 1, 2, ..., 999]
â”œâ”€ Shuffle: [342, 12, 891, ...]
â””â”€ Batches:
   â”œâ”€ Batch 0: indices [342, 12, 891, ..., 156]  (32 examples)
   â”œâ”€ Batch 1: indices [789, 23, 445, ..., 901]  (32 examples)
   ...
   â””â”€ Batch 31: indices [234, 567, ...]          (8 examples)
   
Validation Mode (train=False):
â”œâ”€ indices = [1000, 1001, ..., 1999]
â”œâ”€ NO shuffle (keep same order each time)
â””â”€ Batches:
   â”œâ”€ Batch 0: indices [1000, 1001, ..., 1031]
   ...
```

**Using the Data Loader:**

```python
# Create data
data = SyntheticRegressionData(
    w=torch.tensor([2.0, -3.4]), 
    b=4.2
)

# Get first batch
X, y = next(iter(data.train_dataloader()))

print("X shape:", X.shape)  # torch.Size([32, 2])
print("y shape:", y.shape)  # torch.Size([32, 1])

Explanation:
â”œâ”€ 32 examples in batch
â”œâ”€ 2 features per example
â””â”€ 1 label per example
```

**Full Iteration:**

```python
# Iterate through all batches
for batch_idx, (X, y) in enumerate(data.train_dataloader()):
    print(f"Batch {batch_idx}:")
    print(f"  X shape: {X.shape}")
    print(f"  y shape: {y.shape}")
    
Output:
Batch 0:
  X shape: torch.Size([32, 2])
  y shape: torch.Size([32, 1])
Batch 1:
  X shape: torch.Size([32, 2])
  y shape: torch.Size([32, 1])
...
Batch 31:
  X shape: torch.Size([8, 2])   â† Last batch (partial)
  y shape: torch.Size([8, 1])
```

---

### âš¡ Concise Implementation with PyTorch

**Why Use Built-in Loaders?**

```
Manual Implementation:
â”œâ”€ âŒ Slow (Python loops)
â”œâ”€ âŒ No parallelization
â”œâ”€ âŒ More code to maintain
â”œâ”€ âŒ Must handle edge cases manually
â””â”€ âŒ Not memory efficient

PyTorch DataLoader:
â”œâ”€ âœ… Fast (C++ backend)
â”œâ”€ âœ… Parallel data loading
â”œâ”€ âœ… Minimal code
â”œâ”€ âœ… Handles all edge cases
â””â”€ âœ… Memory efficient
```

**Implementation:**

```python
@add_to_class(DataModule)
def get_tensorloader(self, tensors, train, indices=slice(0, None)):
    """
    Create PyTorch data loader
    
    Args:
        tensors: Tuple of (X, y)
        train: Whether to shuffle
        indices: Which indices to use
    """
    # Select subset of data
    tensors = tuple(a[indices] for a in tensors)
    
    # Create PyTorch dataset
    dataset = torch.utils.data.TensorDataset(*tensors)
    
    # Create data loader
    return torch.utils.data.DataLoader(
        dataset, 
        self.batch_size,
        shuffle=train
    )

@add_to_class(SyntheticRegressionData)
def get_dataloader(self, train):
    """Use built-in PyTorch loader"""
    # Determine which slice to use
    if train:
        i = slice(0, self.num_train)
    else:
        i = slice(self.num_train, None)
    
    return self.get_tensorloader((self.X, self.y), train, i)
```

**Comparison:**

```python
# Both give same results!

# Manual
manual_loader = data.get_dataloader_manual(train=True)

# Built-in
builtin_loader = data.get_dataloader(train=True)

# Same batches
for (X1, y1), (X2, y2) in zip(manual_loader, builtin_loader):
    assert torch.allclose(X1, X2)
    assert torch.allclose(y1, y2)
```

---

### ğŸ“ Data Loader Features

#### **1. Length Support:**

```python
loader = data.train_dataloader()

print(len(loader))  # Number of batches

Calculation:
num_train = 1000
batch_size = 32
num_batches = ceil(1000 / 32) = 32
```

#### **2. Shuffling:**

```python
# First epoch
for X, y in data.train_dataloader():
    # Batch order: random

# Second epoch  
for X, y in data.train_dataloader():
    # DIFFERENT random order

Why shuffle?
â”œâ”€ Prevents model from learning batch order
â”œâ”€ Reduces overfitting
â””â”€ Better gradient estimates
```

#### **3. Parallel Loading:**

```python
loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4  # 4 parallel processes
)

Benefits:
â”œâ”€ Load next batch while training current batch
â”œâ”€ Faster overall training
â””â”€ Better GPU utilization
```

#### **4. Automatic Batching:**

```python
# Handles last batch automatically

If num_train = 1000, batch_size = 32:
â”œâ”€ Batches 0-30: 32 examples each
â””â”€ Batch 31: 8 examples (remainder)

No special code needed!
```

---

### ğŸ” Inspecting the Data

**Individual Examples:**

```python
# First example
print("First example:")
print(f"  Features: {data.X[0]}")
print(f"  Label: {data.y[0]}")

Output:
First example:
  Features: tensor([ 2.2793, -0.2246])
  Label: tensor([9.5014])
  
Interpretation:
â”œâ”€ Feature 1 = 2.2793
â”œâ”€ Feature 2 = -0.2246
â””â”€ Label (price) = $9.5014
```

**Statistics:**

```python
print("Feature statistics:")
print(f"  Mean: {data.X.mean(dim=0)}")
print(f"  Std: {data.X.std(dim=0)}")

Output:
Mean: tensor([-0.0123,  0.0089])  â† Close to 0 âœ…
Std:  tensor([0.9987, 1.0012])    â† Close to 1 âœ…

print("Label statistics:")
print(f"  Mean: {data.y.mean()}")
print(f"  Std: {data.y.std()}")

Interpretation:
â”œâ”€ X generated from N(0, 1) â†’ meanâ‰ˆ0, stdâ‰ˆ1
â””â”€ y depends on w, b, and noise
```

**Verifying Ground Truth:**

```python
# Manually compute expected values
w_true = torch.tensor([2.0, -3.4])
b_true = 4.2

# Predict for first example
x0 = data.X[0]
y_pred = (w_true * x0).sum() + b_true

print(f"Predicted (no noise): {y_pred:.4f}")
print(f"Actual (with noise): {data.y[0].item():.4f}")
print(f"Noise: {(data.y[0] - y_pred).item():.4f}")

Output:
Predicted (no noise): 9.5222
Actual (with noise): 9.5014
Noise: -0.0208  â† Small random variation âœ…
```

---

### ğŸ² Why Random Data Generation Matters

**Advantages:**

```
1. KNOWN GROUND TRUTH:
   â”œâ”€ We set w_true, b_true
   â”œâ”€ Can verify if model recovers them
   â””â”€ Objective measure of success

2. CONTROLLED EXPERIMENTS:
   â”œâ”€ Vary noise level â†’ test robustness
   â”œâ”€ Vary num_features â†’ test scaling
   â”œâ”€ Vary num_samples â†’ test sample efficiency
   â””â”€ Perfect for ablation studies

3. DEBUGGING:
   â”œâ”€ If model can't fit synthetic data â†’ bug in code
   â”œâ”€ If model fits synthetic but not real â†’ data issue
   â””â”€ Isolate problems systematically

4. REPRODUCIBILITY:
   â”œâ”€ Set random seed â†’ same data every time
   â”œâ”€ Others can replicate experiments
   â””â”€ Fair comparison across methods
```

**Setting Random Seeds:**

```python
import torch
import random
import numpy as np

def set_seed(seed=42):
    """Set all random seeds for reproducibility"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    
# Use it
set_seed(42)
data1 = SyntheticRegressionData(w=torch.tensor([2.0, -3.4]), b=4.2)

set_seed(42)
data2 = SyntheticRegressionData(w=torch.tensor([2.0, -3.4]), b=4.2)

# Same data!
assert torch.allclose(data1.X, data2.X)
assert torch.allclose(data1.y, data2.y)
```

---

### ğŸ“Š Data Splits

**Why Split Data?**

```
Full Dataset (2000 examples)
â”‚
â”œâ”€ Training Set (1000 examples)
â”‚  â””â”€ Used to FIT the model
â”‚
â””â”€ Validation Set (1000 examples)
   â””â”€ Used to EVALUATE the model

Critical: Model never sees validation data during training!
```

**Accessing Splits:**

```python
# Training data
train_loader = data.train_dataloader()
print(f"Training batches: {len(train_loader)}")  # 32

# Validation data
val_loader = data.val_dataloader()
print(f"Validation batches: {len(val_loader)}")  # 32

# Different data!
train_X, train_y = next(iter(train_loader))
val_X, val_y = next(iter(val_loader))

# Training uses examples 0-999
# Validation uses examples 1000-1999
```

---

### ğŸ¯ Summary of Chapter 3.3

**What We Learned:**

1. âœ… Why synthetic data is useful (known ground truth)
2. âœ… How to generate linear regression data
3. âœ… Manual data loader implementation
4. âœ… PyTorch built-in data loaders
5. âœ… Train/validation splits
6. âœ… Batching and shuffling

**Key Code:**

```python
# Generate data
data = SyntheticRegressionData(
    w=torch.tensor([2.0, -3.4]),
    b=4.2,
    noise=0.01,
    num_train=1000,
    num_val=1000,
    batch_size=32
)

# Iterate
for X, y in data.train_dataloader():
    # X: (32, 2)
    # y: (32, 1)
    pass
```
# Linear Regression - COMPLETE NOTES (Continued)

---

## ğŸ› ï¸ Chapter 3.4: Linear Regression Implementation from Scratch

### ğŸ¯ Complete Training Flow

```
STEP 1: Initialize Parameters
STEP 2: Define Model (Forward Pass)
STEP 3: Define Loss Function
STEP 4: Define Optimizer
STEP 5: Training Loop
```

---

### **STEP 1: Initialize Parameters**

```python
class LinearRegressionScratch(Module):
    def __init__(self, num_inputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        
        # Initialize weights: w ~ N(0, ÏƒÂ²)
        self.w = torch.normal(0, sigma, (num_inputs, 1), 
                             requires_grad=True)
        
        # Initialize bias: b = 0
        self.b = torch.zeros(1, requires_grad=True)
```

**Why these initializations?**

| Parameter | Initialization | Reason |
|-----------|---------------|---------|
| **Weights (w)** | N(0, 0.01Â²) | âœ… Small random values break symmetry<br>âœ… Not too large (unstable)<br>âœ… Not zero (no learning) |
| **Bias (b)** | 0 | âœ… Common practice<br>âœ… Will be learned anyway |
| **requires_grad** | True | âœ… Enable automatic differentiation<br>âœ… PyTorch tracks gradients |

---

### **STEP 2: Define Model (Forward Pass)**

```python
@add_to_class(LinearRegressionScratch)
def forward(self, X):
    """Compute predictions: Å· = Xw + b"""
    return torch.matmul(X, self.w) + self.b
```

**Matrix multiplication details:**

```
X shape:    (batch_size, num_inputs) = (32, 2)
w shape:    (num_inputs, 1) = (2, 1)
Result:     (32, 1)

Example:
X = [[xâ‚â½Â¹â¾, xâ‚‚â½Â¹â¾],    w = [[wâ‚],    Xw = [[Å·â½Â¹â¾],
     [xâ‚â½Â²â¾, xâ‚‚â½Â²â¾],         [wâ‚‚]]          [Å·â½Â²â¾],
     ...]                                     ...]

Then add b (broadcasts to all rows):
Xw + b = [[Å·â½Â¹â¾ + b],
          [Å·â½Â²â¾ + b],
          ...]
```

---

### **STEP 3: Define Loss Function**

```python
@add_to_class(LinearRegressionScratch)
def loss(self, y_hat, y):
    """Mean Squared Error: (1/n)Î£(Å· - y)Â²/2"""
    l = (y_hat - y) ** 2 / 2
    return l.mean()
```

**Step-by-step:**

```python
y_hat = [[320000],    y = [[300000],
         [450000],         [450000],
         [250000]]         [260000]]

# Element-wise difference
diff = [[20000],      # Error for example 1
        [0],          # Perfect prediction!
        [-10000]]     # Error for example 3

# Square
squared = [[400000000],
           [0],
           [100000000]]

# Divide by 2
halved = [[200000000],
          [0],
          [50000000]]

# Mean
loss = (200000000 + 0 + 50000000) / 3 = 83333333
```

---

### **STEP 4: Define Optimizer (SGD)**

```python
class SGD(HyperParameters):
    def __init__(self, params, lr):
        self.save_hyperparameters()
        
    def step(self):
        """Update parameters: Î¸ â† Î¸ - Î·âˆ‡Î¸"""
        for param in self.params:
            param -= self.lr * param.grad
            
    def zero_grad(self):
        """Clear gradients (must do before backward!)"""
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()

@add_to_class(LinearRegressionScratch)
def configure_optimizers(self):
    return SGD([self.w, self.b], self.lr)
```

**Why zero_grad()?**

```python
# PyTorch ACCUMULATES gradients by default!

Iteration 1:
â”œâ”€ loss.backward() â†’ w.grad = [0.5, 0.3]
â””â”€ optimizer.step() â†’ w -= lr * grad âœ…

Iteration 2 (WITHOUT zero_grad):
â”œâ”€ loss.backward() â†’ w.grad = [0.5, 0.3] + [0.4, 0.2] = [0.9, 0.5]
â””â”€ Wrong! Using accumulated gradients âŒ

Iteration 2 (WITH zero_grad):
â”œâ”€ optimizer.zero_grad() â†’ w.grad = [0, 0]
â”œâ”€ loss.backward() â†’ w.grad = [0.4, 0.2]
â””â”€ optimizer.step() â†’ Correct! âœ…
```

---

### **STEP 5: Training Loop**

```python
@add_to_class(Trainer)
def fit_epoch(self):
    """Train for one epoch"""
    
    # TRAINING PHASE
    self.model.train()  # Set to training mode
    
    for batch in self.train_dataloader:
        # 1. Forward pass
        loss = self.model.training_step(
            self.prepare_batch(batch)
        )
        
        # 2. Backward pass
        self.optim.zero_grad()      # Clear old gradients
        loss.backward()             # Compute new gradients
        
        # 3. Update parameters
        self.optim.step()
        
        self.train_batch_idx += 1
    
    # VALIDATION PHASE
    if self.val_dataloader is None:
        return
        
    self.model.eval()  # Set to evaluation mode
    
    for batch in self.val_dataloader:
        with torch.no_grad():  # Don't compute gradients
            self.model.validation_step(
                self.prepare_batch(batch)
            )
        self.val_batch_idx += 1
```

**Complete Training:**

```python
# Create components
model = LinearRegressionScratch(num_inputs=2, lr=0.03)
data = SyntheticRegressionData(
    w=torch.tensor([2, -3.4]), 
    b=4.2
)
trainer = Trainer(max_epochs=3)

# Train!
trainer.fit(model, data)

# What happens:
# Epoch 1:
#   32 training batches â†’ update parameters 32 times
#   32 validation batches â†’ check performance
# Epoch 2:
#   32 training batches
#   32 validation batches
# Epoch 3:
#   32 training batches
#   32 validation batches
```

---

### ğŸ¯ Training Process Visualization

```
EPOCH 1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training:
  Batch 0:  loss = 10.5  â†’  update w, b
  Batch 1:  loss = 9.8   â†’  update w, b
  ...
  Batch 31: loss = 1.2   â†’  update w, b
  
Validation:
  Batch 0:  loss = 1.5   (just measure)
  Batch 1:  loss = 1.4   (just measure)
  ...
  Batch 31: loss = 1.1   (just measure)
  
  Avg train loss: 5.2
  Avg val loss:   1.3
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EPOCH 2:
  Avg train loss: 0.8  â† Getting better!
  Avg val loss:   0.6
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EPOCH 3:
  Avg train loss: 0.3  â† Even better!
  Avg val loss:   0.2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

### âœ… Checking Results

```python
# After training, compare with ground truth
w_true = torch.tensor([2.0, -3.4])
b_true = 4.2

w_learned = model.w.reshape(w_true.shape)
b_learned = model.b

print(f'True w: {w_true}')
print(f'Learned w: {w_learned}')
print(f'Error: {w_true - w_learned}')

print(f'True b: {b_true}')
print(f'Learned b: {b_learned}')
print(f'Error: {b_true - b_learned}')

# Output:
# True w: tensor([ 2.0000, -3.4000])
# Learned w: tensor([ 1.8837, -3.1980])
# Error: tensor([ 0.1163, -0.2020])  â† Close! âœ…
```

---

## ğŸš€ Chapter 3.5: Concise Implementation with PyTorch

### ğŸ¯ Using Built-in Components

**Comparison:**

| Component | From Scratch | Built-in PyTorch |
|-----------|-------------|------------------|
| **Model** | Manual w, b | `nn.Linear()` |
| **Loss** | Manual MSE | `nn.MSELoss()` |
| **Optimizer** | Custom SGD | `torch.optim.SGD()` |
| **Lines of code** | ~50 | ~15 |

---

### **Model Definition**

```python
class LinearRegression(Module):
    def __init__(self, lr):
        super().__init__()
        self.save_hyperparameters()
        
        # PyTorch layer (automatically creates w and b!)
        self.net = nn.LazyLinear(1)
        
        # Initialize (same as before)
        self.net.weight.data.normal_(0, 0.01)
        self.net.bias.data.fill_(0)
        
    def forward(self, X):
        return self.net(X)
```

**What is `LazyLinear`?**

```python
# Regular Linear:
net = nn.Linear(in_features=2, out_features=1)
# âŒ Must specify input dimension

# LazyLinear:
net = nn.LazyLinear(out_features=1)
# âœ… Input dimension inferred on first forward pass
# âœ… More flexible!

# First forward pass:
X = torch.randn(32, 2)  # 2 input features
output = net(X)          # Now net knows: in_features=2
```

---

### **Loss Function**

```python
@add_to_class(LinearRegression)
def loss(self, y_hat, y):
    fn = nn.MSELoss()
    return fn(y_hat, y)
```

**Built-in vs Manual:**

```python
# Manual:
loss = ((y_hat - y) ** 2 / 2).mean()

# Built-in (no /2 factor):
loss = nn.MSELoss()(y_hat, y)

# Mathematically equivalent for optimization!
# (constant factors don't affect argmin)
```

---

### **Optimizer**

```python
@add_to_class(LinearRegression)
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), self.lr)
```

**`self.parameters()`:**

```python
# Automatically finds ALL learnable parameters!

for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# Output:
# net.weight: torch.Size([1, 2])
# net.bias: torch.Size([1])

# No need to manually track [self.w, self.b] âœ…
```

---

### **Training (Same as Before!)**

```python
model = LinearRegression(lr=0.03)
data = SyntheticRegressionData(
    w=torch.tensor([2, -3.4]), 
    b=4.2
)
trainer = Trainer(max_epochs=3)

trainer.fit(model, data)

# Results:
# Error in w: tensor([ 0.0031, -0.0099])  â† Even better!
# Error in b: tensor([0.0127])
```

---

### ğŸ” Accessing Parameters

```python
@add_to_class(LinearRegression)
def get_w_b(self):
    return (self.net.weight.data, self.net.bias.data)

w, b = model.get_w_b()

# w.shape: (1, 2) - Note: transposed!
# In nn.Linear: output = X @ w^T + b
# So weight matrix is (out_features, in_features)
```

---

## ğŸ“ Chapter 3.6: Generalization

### ğŸ¯ The Core Problem

**Two Students Example:**

```
ELLIE (Memorization):
â”œâ”€ Strategy: Memorize all past exam answers
â”œâ”€ Past exams: 100% âœ…
â””â”€ New exam: 0% âŒ (never seen these questions!)

IRENE (Pattern Learning):
â”œâ”€ Strategy: Understand underlying patterns
â”œâ”€ Past exams: 90% 
â””â”€ New exam: 90% âœ… (patterns still apply!)

We want to be like IRENE!
```

---

### ğŸ“Š Key Definitions

**1. Training Error (Empirical Error):**

```python
# Loss on data we TRAINED on
train_loss = (1/n_train) Î£ loss(model(X_train[i]), y_train[i])

Can measure this! âœ…
```

**2. Generalization Error (True Error):**

```python
# Expected loss on ALL possible data
gen_error = E[loss(model(X), y)]  # X,y ~ true distribution

Cannot measure this exactly! âŒ
(would need infinite data)
```

**3. Validation Error:**

```python
# Loss on data we held out for testing
val_loss = (1/n_val) Î£ loss(model(X_val[i]), y_val[i])

Can measure this! âœ…
Used to ESTIMATE generalization error
```

---

### ğŸ­ Underfitting vs Overfitting

**Underfitting:**

```
Training loss:   HIGH âŒ
Validation loss: HIGH âŒ
Gap:             SMALL

Problem: Model too simple
Solution: 
â”œâ”€ More complex model
â”œâ”€ More features
â””â”€ Train longer
```

**Good Fit:**

```
Training loss:   LOW âœ…
Validation loss: LOW âœ…
Gap:             SMALL âœ…

Sweet spot! This is what we want.
```

**Overfitting:**

```
Training loss:   VERY LOW
Validation loss: HIGH âŒ
Gap:             LARGE âŒ

Problem: Model memorizing training data
Solution:
â”œâ”€ More training data
â”œâ”€ Regularization
â”œâ”€ Simpler model
â””â”€ Early stopping
```

---

### ğŸ“ˆ Polynomial Example

```python
# Fit polynomials of different degrees

Degree 1: y = wâ‚€ + wâ‚x
â”œâ”€ 2 parameters
â”œâ”€ Training error: 5.0
â”œâ”€ Validation error: 5.2
â””â”€ Status: Underfit (too simple)

Degree 3: y = wâ‚€ + wâ‚x + wâ‚‚xÂ² + wâ‚ƒxÂ³
â”œâ”€ 4 parameters
â”œâ”€ Training error: 0.5
â”œâ”€ Validation error: 0.6
â””â”€ Status: Good fit âœ…

Degree 10: y = wâ‚€ + wâ‚x + ... + wâ‚â‚€xÂ¹â°
â”œâ”€ 11 parameters
â”œâ”€ Training error: 0.001
â”œâ”€ Validation error: 100.0
â””â”€ Status: Overfit (memorized training data)
```

**Visual:**

```
Complexity â†’
Low        Medium        High
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Underfit  |  Good  |  Overfit
  
High error  | Sweet  | High val error
both sets   | spot   | (memorization)
```

---

### ğŸ“ Model Selection

**The Three-Way Split:**

```
Full Dataset (10,000 examples)
â”‚
â”œâ”€ Training Set (8,000) - 80%
â”‚  â””â”€ FIT model parameters
â”‚
â”œâ”€ Validation Set (1,000) - 10%
â”‚  â””â”€ SELECT hyperparameters
â”‚
â””â”€ Test Set (1,000) - 10%
   â””â”€ FINAL evaluation (touch ONCE!)
```

**Critical Rules:**

```
âŒ NEVER use test set for:
   â”œâ”€ Choosing model architecture
   â”œâ”€ Tuning hyperparameters
   â”œâ”€ Deciding when to stop training
   â””â”€ ANY decision making!

âœ… ONLY use test set for:
   â””â”€ Final performance report
```

**Why?**

```
If you tune on test set:
â”œâ”€ Model sees test data indirectly
â”œâ”€ Test error becomes optimistic
â”œâ”€ You're overfitting to test set!
â””â”€ Results won't generalize to real world âŒ
```

---

### ğŸ”„ K-Fold Cross-Validation

**When to use:** Not enough data for separate validation set

```
5-Fold Cross-Validation:

Dataset: [1][2][3][4][5]

Fold 1: [T][T][T][T][V]  Train on 1-4, validate on 5
Fold 2: [T][T][T][V][T]  Train on 1-3,5, validate on 4
Fold 3: [T][T][V][T][T]  Train on 1-2,4-5, validate on 3
Fold 4: [T][V][T][T][T]  Train on 1,3-5, validate on 2
Fold 5: [V][T][T][T][T]  Train on 2-5, validate on 1

Final validation error = average of 5 folds
```

**Trade-offs:**

```
Pros:
â”œâ”€ âœ… Uses all data for both training and validation
â”œâ”€ âœ… More reliable estimate with limited data
â””â”€ âœ… Reduces variance in performance estimate

Cons:
â”œâ”€ âŒ K times more expensive (train K models)
â”œâ”€ âŒ Takes K times longer
â””â”€ âŒ More complex implementation
```

---

## âš–ï¸ Chapter 3.7: Weight Decay (L2 Regularization)

### ğŸ¯ The Problem

```python
# High-dimensional, low-sample scenario
num_features = 200
num_training = 20  # Only 20 examples!

Problem:
â”œâ”€ More parameters (200) than data (20)
â”œâ”€ Model can perfectly fit training data
â”œâ”€ But performance on new data is terrible
â””â”€ Extreme overfitting!
```

---

### ğŸ’¡ The Solution: Weight Decay

**Intuition:** Add penalty for large weights

```
Modified Loss Function:
L(w, b) = MSE(w, b) + (Î»/2)||w||Â²
          ï¸¸â”â”â”â”â”â”â”â”â”ï¸¸   ï¸¸â”â”â”â”â”â”â”â”ï¸¸
          Original       Penalty
          loss          term
```

**Components:**

| Symbol | Name | Meaning |
|--------|------|---------|
| **Î» (lambda)** | Regularization strength | How much to penalize large weights |
| **â€–wâ€–Â²** | L2 norm squared | wâ‚Â² + wâ‚‚Â² + ... + wâ‚Â² |
| **/2** | Constant | Makes derivative cleaner |

---

### ğŸ”¢ Why L2 Norm?

**L2 vs L1:**

```python
# L2 Regularization (Ridge):
penalty = Î» * (wâ‚Â² + wâ‚‚Â² + ... + wâ‚Â²)
Effect: All weights shrink proportionally

# L1 Regularization (Lasso):
penalty = Î» * (|wâ‚| + |wâ‚‚| + ... + |wâ‚|)
Effect: Many weights become exactly 0 (sparse)
```

**L2 Benefits:**

```
âœ… Smooth (differentiable everywhere)
âœ… Unique solution
âœ… Distributes weight evenly across features
âœ… More robust to noise
âœ… Easier to optimize
```

---

### ğŸ”„ Updated Training

**Gradient with Weight Decay:**

```
Without weight decay:
âˆ‚L/âˆ‚w = (1/|B|) Î£ x^(i)(Å·^(i) - y^(i))

With weight decay:
âˆ‚L/âˆ‚w = (1/|B|) Î£ x^(i)(Å·^(i) - y^(i)) + Î»w
        ï¸¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”ï¸¸   ï¸¸â”ï¸¸
        Original gradient               Extra term
```

**Update Rule:**

```python
# Standard SGD:
w â† w - Î·âˆ‡L

# With weight decay:
w â† w - Î·(âˆ‡L + Î»w)
  = w - Î·âˆ‡L - Î·Î»w
  = (1 - Î·Î»)w - Î·âˆ‡L
    ï¸¸â”â”â”â”â”â”â”ï¸¸
    Decay factor
```

**Why "Weight Decay"?**

```
Factor (1 - Î·Î») < 1 always shrinks weights:

Example: Î· = 0.01, Î» = 0.1
â”œâ”€ (1 - Î·Î») = (1 - 0.001) = 0.999
â””â”€ Each step: w â† 0.999w - Î·âˆ‡L
              Shrink by 0.1% each iteration
```

---

### ğŸ’» Implementation

**From Scratch:**

```python
def l2_penalty(w):
    return (w ** 2).sum() / 2

class WeightDecayScratch(LinearRegressionScratch):
    def __init__(self, num_inputs, lambd, lr, sigma=0.01):
        super().__init__(num_inputs, lr, sigma)
        self.save_hyperparameters()
        
    def loss(self, y_hat, y):
        # Original loss + penalty
        return (super().loss(y_hat, y) + 
                self.lambd * l2_penalty(self.w))
```

**Using PyTorch:**

```python
class WeightDecay(LinearRegression):
    def __init__(self, wd, lr):
        super().__init__(lr)
        self.wd = wd
        
    def configure_optimizers(self):
        return torch.optim.SGD([
            {'params': self.net.weight, 
             'weight_decay': self.wd},  # Apply to weights
            {'params': self.net.bias}    # NOT to bias
        ], lr=self.lr)
```

---

### ğŸ“Š Effect of Lambda

**Î» = 0 (No Regularization):**

```
Training loss:   0.01  â† Very low
Validation loss: 5.00  â† High!
Gap:             4.99  â† Overfitting!
||w||Â²:          10.5  â† Large weights
```

**Î» = 3 (With Regularization):**

```
Training loss:   0.50  â† Higher
Validation loss: 0.60  â† Much lower!
Gap:             0.10  â† Better generalization âœ…
||w||Â²:          0.15  â† Small weights âœ…
```

---

### ğŸ›ï¸ Choosing Lambda

```
Lambda = 0:
â”œâ”€ No regularization
â””â”€ Risk of overfitting

Lambda small (0.001):
â”œâ”€ Weak regularization
â””â”€ Good if model isn't overfitting

Lambda medium (0.1 - 1):
â”œâ”€ Moderate regularization
â””â”€ Good default choice

Lambda large (10+):
â”œâ”€ Strong regularization
â”œâ”€ Prevents overfitting
â””â”€ Risk of underfitting

Rule: Tune on validation set!
```

---

## ğŸ“ FINAL SUMMARY - Everything Together

### The Complete Linear Regression Pipeline

```
1. DATA PREPARATION
   â”œâ”€ Generate/load data
   â”œâ”€ Split: train/val/test
   â””â”€ Create data loaders

2. MODEL DEFINITION
   â”œâ”€ Initialize parameters (w, b)
   â”œâ”€ Define forward pass
   â””â”€ Define loss function

3. OPTIMIZATION
   â”œâ”€ Choose optimizer (SGD)
   â”œâ”€ Set learning rate
   â””â”€ Add regularization (optional)

4. TRAINING LOOP
   For each epoch:
      â”œâ”€ For each batch:
      â”‚  â”œâ”€ Forward pass
      â”‚  â”œâ”€ Compute loss
      â”‚  â”œâ”€ Backward pass (gradients)
      â”‚  â””â”€ Update parameters
      â””â”€ Validate on validation set

5. EVALUATION
   â”œâ”€ Check on validation set
   â”œâ”€ Tune hyperparameters
   â””â”€ Final test on test set (once!)
```

---

### Key Formulas

```
Model:     Å· = w^T x + b

Loss:      L = (1/n)Î£(Å· - y)Â² + (Î»/2)||w||Â²

Update:    w â† w - Î·(âˆ‡L + Î»w)
           b â† b - Î·âˆ‡L
```

---

### Important Concepts

| Concept | Simple Explanation |
|---------|-------------------|
| **Overfitting** | Model memorizes training data, fails on new data |
| **Underfitting** | Model too simple, fails on everything |
| **Regularization** | Add penalty to prevent overfitting |
| **Weight Decay** | Shrink weights toward zero |
| **Learning Rate** | How big each update step is |
| **Batch Size** | How many examples to process together |
| **Validation** | Separate data to check generalization |

---

### Critical Rules âš ï¸

```
âœ… DO:
â”œâ”€ Always split train/val/test
â”œâ”€ Shuffle training data each epoch
â”œâ”€ Monitor validation loss
â”œâ”€ Use regularization if overfitting
â”œâ”€ Normalize/standardize features
â””â”€ Set random seeds for reproducibility

âŒ DON'T:
â”œâ”€ Train on test set
â”œâ”€ Tune hyperparameters on test set
â”œâ”€ Forget to zero gradients
â”œâ”€ Use learning rate too large
â”œâ”€ Ignore validation error
â””â”€ Skip data shuffling
 