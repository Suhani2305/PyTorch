# Linear Neural Networks for Classification  
 

## ğŸ“š Chapter 4.1: Softmax Regression

### ğŸ¯ Classification vs Regression

**The Big Difference:**

| Regression | Classification |
|------------|----------------|
| **Question:** "How much?" | **Question:** "Which category?" |
| **Output:** Continuous number | **Output:** Discrete category |
| **Example:** House price = $320,000 | **Example:** Email = "Spam" or "Inbox" |
| **Loss:** MSE (Mean Squared Error) | **Loss:** Cross-Entropy |
| **Output layer:** Single neuron | **Output layer:** Multiple neurons (one per class) |

---

### ğŸ“§ Real-World Classification Problems

```
1. EMAIL FILTERING
   Input: Email text, sender, subject
   Output: Spam | Inbox | Social | Promotions
   
2. IMAGE CLASSIFICATION
   Input: Pixel values
   Output: Dog | Cat | Bird | Car | ...
   
3. MEDICAL DIAGNOSIS
   Input: Symptoms, test results
   Output: Disease A | Disease B | Healthy
   
4. SENTIMENT ANALYSIS
   Input: Review text
   Output: Positive | Negative | Neutral
   
5. FRAUD DETECTION
   Input: Transaction details
   Output: Fraudulent | Legitimate
```

---

### ğŸ·ï¸ Two Types of Classification

**1. Hard Classification (What we usually want):**
```
Give me ONE definite answer
Example: This email IS spam (100% decision)
```

**2. Soft Classification (How models actually work):**
```
Give me probabilities for each class
Example: 
â”œâ”€ P(Spam) = 0.92
â”œâ”€ P(Inbox) = 0.05
â””â”€ P(Social) = 0.03
```

**Why soft first, then hard?**
- Model outputs probabilities (soft)
- We pick highest probability (convert to hard)
- Allows us to set confidence thresholds

---

### ğŸ”¢ Label Encoding - Two Approaches

**Problem:** Computers need numbers, not words

#### **âŒ Bad Approach: Integer Encoding**

```python
Labels: "cat", "chicken", "dog"
Encode: cat=0, chicken=1, dog=2

Problem:
â”œâ”€ Implies ordering: cat < chicken < dog
â”œâ”€ Implies distance: dog-cat = 2, chicken-cat = 1
â””â”€ Model might think: dog = 2Ã—cat ???

When it's OK:
âœ… Ordinal data: baby < toddler < adult < geriatric
```

#### **âœ… Good Approach: One-Hot Encoding**

```python
Classes: cat, chicken, dog (3 classes)

One-Hot Vectors:
cat     = [1, 0, 0]  â† 1st position = 1, rest = 0
chicken = [0, 1, 0]  â† 2nd position = 1, rest = 0
dog     = [0, 0, 1]  â† 3rd position = 1, rest = 0

Benefits:
âœ… No implied ordering
âœ… No implied distances
âœ… Each class treated equally
âœ… Works with any loss function
```

**General Pattern:**

```
q classes â†’ q-dimensional vector
Only one element = 1, all others = 0

Position of '1' indicates the class
```

---

### ğŸ§  Network Architecture

**Input:** 2Ã—2 grayscale image (4 pixels)

```
Flatten: [xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„]

Network Structure:

Input Layer (4 neurons):
    xâ‚ â”€â”€â”€â”€â”€â”
    xâ‚‚ â”€â”€â”€â”€â”€â”¤
    xâ‚ƒ â”€â”€â”€â”€â”€â”¤
    xâ‚„ â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    (Fully Connected)
            â”‚
            â†“
Output Layer (3 neurons):
    oâ‚ â†’ P(cat)
    oâ‚‚ â†’ P(chicken)  
    oâ‚ƒ â†’ P(dog)
```

**Mathematical Form:**

```
oâ‚ = xâ‚wâ‚â‚ + xâ‚‚wâ‚‚â‚ + xâ‚ƒwâ‚ƒâ‚ + xâ‚„wâ‚„â‚ + bâ‚
oâ‚‚ = xâ‚wâ‚â‚‚ + xâ‚‚wâ‚‚â‚‚ + xâ‚ƒwâ‚ƒâ‚‚ + xâ‚„wâ‚„â‚‚ + bâ‚‚
oâ‚ƒ = xâ‚wâ‚â‚ƒ + xâ‚‚wâ‚‚â‚ƒ + xâ‚ƒwâ‚ƒâ‚ƒ + xâ‚„wâ‚„â‚ƒ + bâ‚ƒ

Compact form:
o = Xw + b

Where:
X: (batch_size Ã— 4) - Input features
w: (4 Ã— 3) - Weight matrix
b: (3,) - Bias vector
o: (batch_size Ã— 3) - Output logits
```

**Parameter Count:**

```
Input features (d) = 4
Output classes (q) = 3

Weights: d Ã— q = 4 Ã— 3 = 12 parameters
Biases:  q = 3 parameters
Total: 15 parameters
```

---

### ğŸŒ¡ï¸ The Softmax Function - CRITICAL!

**Problem with raw outputs (logits):**

```python
o = [2.5, -1.0, 0.3]

Problems:
âŒ Can be negative: -1.0
âŒ Don't sum to 1: 2.5 + (-1.0) + 0.3 = 1.8 â‰  1
âŒ Can't interpret as probabilities
```

**Solution: Softmax Transformation**

```
Step 1: Exponentiate (make positive)
exp(o) = [exp(2.5), exp(-1.0), exp(0.3)]
       = [12.18, 0.37, 1.35]

Step 2: Normalize (make sum to 1)
softmax(o) = exp(o) / sum(exp(o))
           = [12.18, 0.37, 1.35] / 13.90
           = [0.876, 0.027, 0.097]
           
Final probabilities:
â”œâ”€ P(cat) = 0.876 = 87.6%  â† Highest! This is our prediction
â”œâ”€ P(chicken) = 0.027 = 2.7%
â””â”€ P(dog) = 0.097 = 9.7%
```

**General Formula:**

```
For output oâ±¼:

softmax(o)â±¼ = exp(oâ±¼) / Î£â‚– exp(oâ‚–)

Properties:
âœ… All values > 0 (exponential is always positive)
âœ… All values < 1 (normalized)
âœ… Sum to 1 (Î£ softmax(o)â±¼ = 1)
âœ… Preserves ordering (if oâ‚ > oâ‚‚, then softmax(o)â‚ > softmax(o)â‚‚)
```

---

### ğŸ”¥ Softmax Properties - Deep Dive

#### **1. Monotonicity:**

```python
o = [1.0, 2.0, 3.0]
softmax(o) = [0.09, 0.24, 0.67]

Increase o[2]:
o = [1.0, 2.0, 5.0]
softmax(o) = [0.02, 0.05, 0.93]  â† oâ‚ƒ increased â†’ Pâ‚ƒ increased
```

#### **2. Order Preservation:**

```python
o = [3.0, 1.0, 2.0]
     â†‘        â†‘
   Largest  Smallest

softmax(o) = [0.67, 0.09, 0.24]
               â†‘         â†‘
            Largest   Smallest

argmax(o) = argmax(softmax(o)) = 0
```

**Why this matters:**
```
For prediction, we only need to find argmax(o)
Don't actually need to compute softmax!
Just pick the largest logit âœ…
```

#### **3. Translation Invariance:**

```python
o = [1, 2, 3]
o' = [2, 3, 4]  # Added 1 to all
o'' = [0, 1, 2]  # Subtracted 1 from all

softmax(o) = softmax(o') = softmax(o'') 
           = [0.09, 0.24, 0.67]

Why? 
exp(oâ±¼ - c) / Î£â‚– exp(oâ‚– - c) 
= exp(oâ±¼)Â·exp(-c) / (Î£â‚– exp(oâ‚–)Â·exp(-c))
= exp(oâ±¼) / Î£â‚– exp(oâ‚–)  âœ…
```

**Numerical Stability Trick:**

```python
# BAD (can overflow):
o = [1000, 1001, 1002]
exp(1000) = âˆ  ğŸ’¥

# GOOD (stable):
o_max = max(o) = 1002
o_stable = o - o_max = [-2, -1, 0]
exp(o_stable) = [0.135, 0.368, 1.0]  âœ…
```

---

### ğŸ“Š Loss Function: Cross-Entropy

**Why not use Mean Squared Error?**

```
MSE for classification:
y_true = [1, 0, 0]  (cat)
y_pred = [0.7, 0.2, 0.1]

MSE = mean((y_true - y_pred)Â²)
    = mean([(1-0.7)Â², (0-0.2)Â², (0-0.1)Â²])
    = mean([0.09, 0.04, 0.01])
    = 0.047

Problems:
âŒ Doesn't penalize confident wrong predictions enough
âŒ Treats all wrong predictions similarly
âŒ Not derived from probability theory
```

**Cross-Entropy Loss - The Right Choice:**

```
Formula:
l(Å·, y) = -Î£â±¼ yâ±¼ log(Å·â±¼)

For one-hot y (only one yâ±¼ = 1):
l(Å·, y) = -log(Å·_true_class)
```

**Detailed Example:**

```python
# True label: cat
y = [1, 0, 0]

# Prediction 1: Confident and correct
Å·â‚ = [0.9, 0.05, 0.05]
lâ‚ = -log(0.9) = 0.105  â† Low loss âœ…

# Prediction 2: Not confident but correct
Å·â‚‚ = [0.4, 0.3, 0.3]
lâ‚‚ = -log(0.4) = 0.916  â† Higher loss

# Prediction 3: Confident but WRONG
Å·â‚ƒ = [0.1, 0.8, 0.1]
lâ‚ƒ = -log(0.1) = 2.303  â† Very high loss! âŒ

# Prediction 4: Extremely confident but WRONG
Å·â‚„ = [0.01, 0.98, 0.01]
lâ‚„ = -log(0.01) = 4.605  â† Massive loss! âŒâŒ
```

**Why Cross-Entropy Works:**

```
Good Prediction:
â”œâ”€ High probability for correct class
â”œâ”€ log(0.99) â‰ˆ -0.01
â””â”€ Loss â‰ˆ 0 âœ…

Bad Prediction:
â”œâ”€ Low probability for correct class
â”œâ”€ log(0.01) â‰ˆ -4.6
â””â”€ Loss very high âŒ

Terrible Prediction:
â”œâ”€ Probability â†’ 0
â”œâ”€ log(0.0001) â‰ˆ -9.2
â””â”€ Loss â†’ âˆ âŒâŒâŒ
```

---

### ğŸ“ Cross-Entropy Derivation

**Starting Point:**

```
We have:
â”œâ”€ True label: y (one-hot)
â”œâ”€ Predictions: Å· = softmax(o)
â””â”€ Want: Loss function

Cross-entropy between distributions:
H(P, Q) = -Î£â±¼ P(j) log Q(j)

In our case:
â”œâ”€ P = true distribution = y (one-hot)
â”œâ”€ Q = predicted distribution = Å·
â””â”€ H(y, Å·) = -Î£â±¼ yâ±¼ log(Å·â±¼)
```

**Simplification for One-Hot:**

```
y = [0, 0, 1, 0]  (class 3 is correct)

Full formula:
H(y, Å·) = -[0Â·log(Å·â‚) + 0Â·log(Å·â‚‚) + 1Â·log(Å·â‚ƒ) + 0Â·log(Å·â‚„)]
        = -log(Å·â‚ƒ)

Result: Only the probability assigned to TRUE class matters!
```

---

### ğŸ“ Information Theory Connection

**Key Concepts:**

#### **1. Entropy (Uncertainty):**

```
H(P) = -Î£â±¼ P(j) log P(j)

Intuition: How much "surprise" or "information" in distribution P

Examples:
â”œâ”€ Coin flip (fair): P = [0.5, 0.5]
â”‚  H(P) = -[0.5 log(0.5) + 0.5 log(0.5)] = 0.693
â”‚  â†‘ Maximum uncertainty
â”‚
â”œâ”€ Biased coin: P = [0.9, 0.1]
â”‚  H(P) = -[0.9 log(0.9) + 0.1 log(0.1)] = 0.325
â”‚  â†‘ Less uncertainty
â”‚
â””â”€ Certain: P = [1.0, 0.0]
   H(P) = -[1.0 log(1.0) + 0 log(0)] = 0
   â†‘ Zero uncertainty (no surprise)
```

#### **2. Cross-Entropy (Prediction Cost):**

```
H(P, Q) = -Î£â±¼ P(j) log Q(j)

Intuition: Cost of encoding P using code optimized for Q

Example:
True: P = [1, 0, 0] (definitely cat)
Pred: Q = [0.7, 0.2, 0.1]

H(P, Q) = -[1Â·log(0.7) + 0Â·log(0.2) + 0Â·log(0.1)]
        = -log(0.7) = 0.357

If we predicted perfectly:
Q = [1, 0, 0]
H(P, Q) = -log(1) = 0  â† Minimum possible!
```

**Key Property:**
```
H(P, Q) â‰¥ H(P)
Equality only when P = Q

In words: 
Cross-entropy is minimized when prediction matches truth
```

---

### ğŸ”¬ Softmax + Cross-Entropy Math

**Combined Gradient (IMPORTANT!):**

```
Loss: l = -Î£â±¼ yâ±¼ log(Å·â±¼)
where Å·â±¼ = softmax(o)â±¼ = exp(oâ±¼) / Î£â‚– exp(oâ‚–)

Gradient:
âˆ‚l/âˆ‚oâ±¼ = softmax(o)â±¼ - yâ±¼
       = Å·â±¼ - yâ±¼

BEAUTIFUL RESULT! 
Just like linear regression: (prediction - truth)
```

**Example Calculation:**

```python
y = [1, 0, 0]  # True: cat
o = [2.0, -1.0, 0.5]  # Raw outputs

# Forward pass
Å· = softmax(o) = [0.73, 0.04, 0.23]

# Loss
l = -log(0.73) = 0.315

# Gradient
âˆ‚l/âˆ‚o = Å· - y 
      = [0.73-1, 0.04-0, 0.23-0]
      = [-0.27, 0.04, 0.23]

Interpretation:
â”œâ”€ oâ‚ should DECREASE (predicted too low)
â”œâ”€ oâ‚‚ should increase slightly
â””â”€ oâ‚ƒ should increase
```

---

### ğŸ“Š Multi-Class Output Layer

**Architecture:**

```
Input: d features
Output: q classes

Weight Matrix W: (d Ã— q)
Bias Vector b: (q,)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input    Weights    Output         â”‚
â”‚                                      â”‚
â”‚  xâ‚ â”€â”€â”€ wâ‚â‚,wâ‚â‚‚,wâ‚â‚ƒ â”€â”€â†’ oâ‚         â”‚
â”‚  xâ‚‚ â”€â”€â”€ wâ‚‚â‚,wâ‚‚â‚‚,wâ‚‚â‚ƒ â”€â”€â†’ oâ‚‚         â”‚
â”‚  xâ‚ƒ â”€â”€â”€ wâ‚ƒâ‚,wâ‚ƒâ‚‚,wâ‚ƒâ‚ƒ â”€â”€â†’ oâ‚ƒ         â”‚
â”‚  xâ‚„ â”€â”€â”€ wâ‚„â‚,wâ‚„â‚‚,wâ‚„â‚ƒ                 â”‚
â”‚         â””â”€ Each output connected    â”‚
â”‚            to ALL inputs             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output j receives contribution from ALL inputs:
oâ±¼ = Î£áµ¢ xáµ¢wáµ¢â±¼ + bâ±¼
```

**Vectorized:**

```python
# For single example:
o = x @ W + b

# For batch (n examples):
O = X @ W + b

Shapes:
X: (n, d)
W: (d, q)  
b: (q,)
O: (n, q)  â† Broadcasting adds b to each row
```

---

## ğŸ“· Chapter 4.2: Fashion-MNIST Dataset

### ğŸ¯ Why Fashion-MNIST?

**Historical Context:**

| Dataset | Released | Size | Difficulty | Status |
|---------|----------|------|------------|--------|
| **MNIST** | 1998 | 60k train | Too easy now | Retired âŒ |
| **Fashion-MNIST** | 2017 | 60k train | Good for learning | Current âœ… |
| **ImageNet** | 2009 | 1.2M train | Too hard for tutorials | Research |

**Why not MNIST anymore?**
```
MNIST (handwritten digits):
â”œâ”€ Simple models get 95%+ accuracy
â”œâ”€ Can't distinguish good vs great models
â”œâ”€ Not representative of real problems
â””â”€ Too easy!

Fashion-MNIST (clothing items):
â”œâ”€ Same size/format as MNIST
â”œâ”€ More challenging (harder to classify)
â”œâ”€ Better for learning
â””â”€ Still manageable for tutorials âœ…
```

---

### ğŸ“¦ Dataset Details

**10 Categories:**

```python
0: T-shirt/top
1: Trouser
2: Pullover
3: Dress
4: Coat
5: Sandal
6: Shirt
7: Sneaker
8: Bag
9: Ankle boot
```

**Dataset Split:**

```
Training Set:   60,000 images (6,000 per class)
Test Set:       10,000 images (1,000 per class)

Image Format:
â”œâ”€ Grayscale (1 channel)
â”œâ”€ 28Ã—28 pixels (can resize to 32Ã—32)
â””â”€ Values: 0-255 (normalized to 0-1)
```

**Tensor Shapes:**

```python
Single image: (1, 28, 28)
               â†‘   â†‘   â†‘
            channels height width

Batch: (batch_size, 1, 28, 28)
        â†‘           â†‘  â†‘   â†‘
      examples  channels H  W

Example: batch_size=64
Shape: (64, 1, 28, 28)
```

---

### ğŸ’» Loading the Dataset

```python
class FashionMNIST(DataModule):
    def __init__(self, batch_size=64, resize=(28, 28)):
        super().__init__()
        self.save_hyperparameters()
        
        # Transformations
        trans = transforms.Compose([
            transforms.Resize(resize),    # Resize if needed
            transforms.ToTensor()         # Convert to tensor (0-1)
        ])
        
        # Download and load
        self.train = torchvision.datasets.FashionMNIST(
            root=self.root,
            train=True,        # Training set
            transform=trans,
            download=True      # Auto-download if needed
        )
        
        self.val = torchvision.datasets.FashionMNIST(
            root=self.root,
            train=False,       # Test set (we use as validation)
            transform=trans,
            download=True
        )
```

---

### ğŸ”„ Data Loading

```python
data = FashionMNIST(batch_size=64)

# Get one batch
X, y = next(iter(data.train_dataloader()))

print(X.shape)  # torch.Size([64, 1, 28, 28])
print(y.shape)  # torch.Size([64])

X: Images (64 images, 1 channel, 28Ã—28 pixels)
y: Labels (64 integers, each 0-9)
```

**Label Examples:**

```python
y = tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, ...])
           â†‘  â†‘  â†‘  â†‘  
        boot shirt shirt dress ...

# Convert to text
labels = data.text_labels(y)
# ['ankle boot', 't-shirt', 't-shirt', 'dress', ...]
```

---

## ğŸ—ï¸ Chapter 4.3: Base Classification Model

### ğŸ¯ Classifier Class

**Key Difference from Regression:**

```python
class Classifier(Module):
    """
    Base for all classification models
    
    New features vs regression:
    â”œâ”€ Accuracy metric (in addition to loss)
    â”œâ”€ Argmax for predictions
    â””â”€ Special validation step
    """
    
    def validation_step(self, batch):
        Y_hat = self(*batch[:-1])
        
        # Plot BOTH loss and accuracy
        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)
        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)
```

---

### ğŸ¯ Accuracy Metric - CRITICAL FOR INTERVIEWS!

**Definition:**

```
Accuracy = (Number of correct predictions) / (Total predictions)
         = (TP + TN) / (TP + TN + FP + FN)

Where:
TP = True Positives
TN = True Negatives  
FP = False Positives
FN = False Negatives
```

**Implementation:**

```python
def accuracy(self, Y_hat, Y, averaged=True):
    """
    Compute accuracy
    
    Args:
        Y_hat: Predictions (batch_size, num_classes)
        Y: True labels (batch_size,)
        averaged: Return mean or individual results
    """
    # Reshape to (batch_size, num_classes)
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    
    # Get predicted class (argmax)
    preds = Y_hat.argmax(axis=1).type(Y.dtype)
    
    # Compare with truth
    compare = (preds == Y.reshape(-1)).type(torch.float32)
    
    # Return mean or individual
    return compare.mean() if averaged else compare
```

**Step-by-Step Example:**

```python
Y_hat = [[0.1, 0.7, 0.2],   # Pred: class 1
         [0.8, 0.1, 0.1],   # Pred: class 0
         [0.2, 0.3, 0.5]]   # Pred: class 2

Y = [1, 0, 1]  # True labels

# Step 1: Argmax
preds = [1, 0, 2]

# Step 2: Compare
compare = [1==1, 0==0, 2==1]
        = [True, True, False]
        = [1.0, 1.0, 0.0]

# Step 3: Average
accuracy = (1.0 + 1.0 + 0.0) / 3 = 0.667 = 66.7%
```

---

### ğŸ“Š Why Track Both Loss and Accuracy?

```
LOSS:
â”œâ”€ Differentiable âœ…
â”œâ”€ Used for optimization
â”œâ”€ Sensitive to confidence
â””â”€ Can decrease even if accuracy stays same

ACCURACY:
â”œâ”€ Not differentiable âŒ
â”œâ”€ Cannot optimize directly
â”œâ”€ What we actually care about
â””â”€ Easy to interpret

Example scenario:
Epoch 1: Loss=1.5, Acc=75%
Epoch 2: Loss=0.8, Acc=75%  â† Loss improved, accuracy same
Epoch 3: Loss=0.5, Acc=80%  â† Both improved!

Tracking both gives complete picture
```

---

## ğŸ”¨ Chapter 4.4: Softmax Implementation from Scratch

### ğŸ§® Softmax Implementation

```python
def softmax(X):
    """
    Compute softmax
    
    Args:
        X: (batch_size, num_classes)
    
    Returns:
        Probabilities: (batch_size, num_classes)
    """
    # Step 1: Exponentiate
    X_exp = torch.exp(X)
    
    # Step 2: Sum across classes (axis=1)
    partition = X_exp.sum(1, keepdims=True)
    
    # Step 3: Normalize
    return X_exp / partition  # Broadcasting
```

**Detailed Example:**

```python
X = [[1.0, 2.0, 3.0],
     [0.1, 0.2, 0.7]]

# Step 1: Exp
X_exp = [[2.72, 7.39, 20.09],
         [1.11, 1.22, 2.01]]

# Step 2: Sum per row
partition = [[30.20],  # Row 1 sum
             [4.34]]   # Row 2 sum

# Step 3: Divide
result = [[2.72/30.20, 7.39/30.20, 20.09/30.20],
          [1.11/4.34, 1.22/4.34, 2.01/4.34]]
       = [[0.09, 0.24, 0.67],
          [0.26, 0.28, 0.46]]

# Verify: Each row sums to 1
[0.09+0.24+0.67, 0.26+0.28+0.46] = [1.0, 1.0] âœ…
```

---

### ğŸ—ï¸ Model Architecture

```python
class SoftmaxRegressionScratch(Classifier):
    def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
        super().__init__()
        self.save_hyperparameters()
        
        # Initialize parameters
        self.W = torch.normal(
            0, sigma, 
            size=(num_inputs, num_outputs),
            requires_grad=True
        )
        self.b = torch.zeros(num_outputs, requires_grad=True)
```

**For Fashion-MNIST:**

```
num_inputs = 28 Ã— 28 = 784  (flattened image)
num_outputs = 10             (10 classes)

W: (784, 10) â†’ 7,840 parameters
b: (10,)     â†’ 10 parameters
Total: 7,850 parameters
```

---

### ğŸ”„ Forward Pass

```python
def forward(self, X):
    """
    X: (batch_size, 1, 28, 28) - Images
    Output: (batch_size, 10) - Probabilities
    """
    # Step 1: Flatten
    X = X.reshape((-1, self.W.shape[0]))
    # Now: (batch_size, 784)
    
    # Step 2: Linear transformation
    O = torch.matmul(X, self.W) + self.b
    # Now: (batch_size, 10) - Logits
    
    # Step 3: Softmax
    return softmax(O)
    # Final: (batch_size, 10) - Probabilities
```

**Visual Flow:**

```
Input: (64, 1, 28, 28)
   â†“
Flatten: (64, 784)
   â†“
X @ W: (64, 784) @ (784, 10) = (64, 10)
   â†“
Add b: (64, 10) + (10,) = (64, 10)  [broadcasting]
   â†“
Softmax: (64, 10) â†’ (64, 10)
   â†“
Output: Each row is a probability distribution
```

---

### ğŸ’¥ Cross-Entropy Implementation

```python
def cross_entropy(y_hat, y):
    """
    y_hat: (batch_size, num_classes) - Probabilities
    y: (batch_size,) - True class indices
    """
    # Select probability of true class for each example
    return -torch.log(y_hat[range(len(y_hat)), y]).mean()
```

**Indexing Trick Explained:**

```python
y_hat = [[0.1, 0.3, 0.6],   # Example 0
         [0.3, 0.2, 0.5]]   # Example 1

y = [0, 2]  # True classes

# What we want:
# Example 0: probability of class 0 = 0.1
# Example 1: probability of class 2 = 0.5

# Fancy indexing:
y_hat[[0, 1], y] = y_hat[[0, 1], [0, 2]]
                 = [y_hat[0, 0], y_hat[1, 2]]
                 = [0.1, 0.5]  âœ…

# Loss
loss = -log([0.1, 0.5]).mean()
     = -[log(0.1) + log(0.5)] / 2
     = -[-2.303 + -0.693] / 2
     = 1.498
```

---

### ğŸ‹ï¸ Training

```python
# Setup
data = FashionMNIST(batch_size=256)
model = SoftmaxRegressionScratch(
    num_inputs=784,   # 28Ã—28
    num_outputs=10,   # 10 classes
    lr=0.1
)
trainer = Trainer(max_epochs=10)

# Train
trainer.fit(model, data)

Results after 10 epochs:
â”œâ”€ Training accuracy: ~82%
â””â”€ Validation accuracy: ~80%
```

---

### ğŸ¯ Making Predictions

```python
# Get test batch
X, y = next(iter(data.val_dataloader()))

# Forward pass
probs = model(X)  # (256, 10)

# Get predicted classes
preds = probs.argmax(axis=1)  # (256,)

# Example:
probs[0] = [0.01, 0.05, 0.02, 0.03, 0.01, 
            0.70, 0.10, 0.05, 0.02, 0.01]
            â†‘    â†‘    â†‘    â†‘    â†‘    â†‘
           0    1    2    3    4    5 â† Predicted!

preds[0] = 5  # Sandal
y[0] = 5      # True label
Correct! âœ…
```

---

### ğŸ” Analyzing Errors

```python
# Find wrong predictions
wrong = (preds != y)

# Get wrong examples
X_wrong = X[wrong]
y_wrong = y[wrong]
preds_wrong = preds[wrong]

# Visualize
# True: "sneaker"
# Pred: "ankle boot"  â† Easy to confuse!

# True: "shirt"
# Pred: "t-shirt"     â† Very similar!

# True: "pullover"
# Pred: "coat"        â† Reasonable mistake
```

**Common Confusions:**

```
Often Confused:
â”œâ”€ Shirt â†” T-shirt
â”œâ”€ Pullover â†” Coat
â”œâ”€ Sneaker â†” Ankle boot
â””â”€ Dress â†” Coat

Rarely Confused:
â”œâ”€ Bag â†” Trouser (very different!)
â”œâ”€ Sandal â†” Shirt
â””â”€ Sneaker â†” Bag
```

---

## ğŸš€ Chapter 4.5: Concise Implementation

### âš¡ Using PyTorch Built-ins

```python
class SoftmaxRegression(Classifier):
    def __init__(self, num_outputs, lr):
        super().__init__()
        self.save_hyperparameters()
        
        self.net = nn.Sequential(
            nn.Flatten(),              # (64,1,28,28) â†’ (64,784)
            nn.LazyLinear(num_outputs) # (64,784) â†’ (64,10)
        )
        
    def forward(self, X):
        return self.net(X)
```

**Key Components:**

```python
nn.Flatten():
â”œâ”€ Converts (batch, channels, H, W) 
â””â”€ To (batch, channelsÃ—HÃ—W)

Example:
Input:  (64, 1, 28, 28)
Output: (64, 784)

nn.LazyLinear(10):
â”œâ”€ Automatically determines input size
â”œâ”€ Creates weight matrix (784, 10)
â””â”€ Creates bias vector (10,)
```

---

### ğŸ”’ Numerical Stability - IMPORTANT!

**The Problem:**

```python
# Naive softmax
o = [1000, 1001, 1002]  # Large values!

exp(1000) â‰ˆ 10^434  ğŸ’¥ OVERFLOW!
Result: inf, nan

# Also problematic:
o = [-1000, -1001, -1002]  # Very negative

exp(-1000) â‰ˆ 10^-434  ğŸ’¥ UNDERFLOW!
Result: 0, 0, 0
Then log(0) = -âˆ  ğŸ’¥
```

**The Solution: LogSumExp Trick**

```python
# Built-in PyTorch handles this!
loss = F.cross_entropy(logits, labels)

# What it does internally:
# Instead of: log(softmax(o))
# Computes: o - log(Î£ exp(o))

# Stable version:
o_max = o.max()
log_softmax = o - o_max - log(sum(exp(o - o_max)))

Benefits:
âœ… Avoids computing exp of large numbers
âœ… Avoids log of very small numbers
âœ… Numerically stable
```

---

### ğŸ¯ Complete Built-in Loss

```python
@add_to_class(Classifier)
def loss(self, Y_hat, Y, averaged=True):
    """
    Y_hat: (batch_size, num_classes) - LOGITS (not probabilities!)
    Y: (batch_size,) - True class indices
    """
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    Y = Y.reshape((-1,))
    
    return F.cross_entropy(
        Y_hat, Y,
        reduction='mean' if averaged else 'none'
    )
```

**Key Point:**

```
F.cross_entropy expects LOGITS, not probabilities!

DON'T do:
probs = softmax(logits)
loss = F.cross_entropy(probs, labels)  âŒ

DO:
loss = F.cross_entropy(logits, labels)  âœ…
(Softmax is computed internally)
```

---

### ğŸƒ Training Results

```python
model = SoftmaxRegression(num_outputs=10, lr=0.1)
trainer.fit(model, data)

Typical results:
Epoch 1:  Train=78%, Val=77%
Epoch 5:  Train=82%, Val=81%
Epoch 10: Train=83%, Val=82%

Observations:
âœ… Validation close to training (good generalization)
âœ… Steady improvement
âœ… Converges quickly
```

---

## ğŸ“Š Chapter 4.6: Generalization in Classification

### ğŸ¯ The Fundamental Question

```
We train on 60,000 images
We test on 10,000 images

Question: Will our model work on the NEXT million images?

This is the generalization problem!
```

---

### ğŸ“ Test Set Statistics

**Training Error vs Test Error:**

```
Training Error (R_emp):
â”œâ”€ Computed on training data
â”œâ”€ Can measure exactly
â””â”€ Formula: (1/n) Î£ loss(f(xáµ¢), yáµ¢)

Population Error (R):
â”œâ”€ True error on entire population
â”œâ”€ CANNOT measure (infinite data)
â””â”€ Formula: E[loss(f(X), Y)]

Test Error (Îµ):
â”œâ”€ Computed on test data
â”œâ”€ ESTIMATES population error
â””â”€ Formula: (1/m) Î£ loss(f(x'áµ¢), y'áµ¢)
```

---

### ğŸ“Š How Many Test Samples Needed?

**Central Limit Theorem Application:**

```
Test error converges at rate: 1/âˆšn

To halve the error estimate uncertainty:
â”œâ”€ Need 4Ã— more samples

To reduce by factor of 10:
â”œâ”€ Need 100Ã— more samples

Example:
Want: 95% confidence that |Îµ - R| < 0.01

Formula: n â‰ˆ 10,000 samples needed!

This is why:
â”œâ”€ MNIST test set = 10,000 âœ…
â”œâ”€ ImageNet test set = 50,000 âœ…
â””â”€ Standard practice in ML
```

---

### âš ï¸ Test Set Reuse Problem

**The Danger:**

```
Round 1:
â”œâ”€ Train model fâ‚
â”œâ”€ Test on test set
â””â”€ Accuracy: 80%

Round 2:
â”œâ”€ Train model fâ‚‚ (different architecture)
â”œâ”€ Test on SAME test set
â””â”€ Accuracy: 82%  â† Is this real improvement?

Round 3:
â”œâ”€ Train model fâ‚ƒ
â”œâ”€ Test on SAME test set again
â””â”€ Accuracy: 85%  â† Can we trust this?

Problem: TEST SET CONTAMINATION!
```

**Why This is Bad:**

```
Multiple Testing Problem:
â”œâ”€ Test 1 model: 5% chance of false positive
â”œâ”€ Test 20 models: 64% chance at least one is misleading!
â”œâ”€ Test 100 models: 99.4% chance of contamination
â””â”€ P(at least one false positive) = 1 - 0.95^k

Information Leakage:
â”œâ”€ You saw test results
â”œâ”€ You modified model based on test results
â”œâ”€ Test set is no longer "unseen"
â””â”€ Overestimate true performance
```

---

### âœ… Correct Practice

```
PROPER WORKFLOW:

1. Split Data:
   â”œâ”€ Training: 60%
   â”œâ”€ Validation: 20%
   â””â”€ Test: 20%

2. Model Development:
   â”œâ”€ Train on training set
   â”œâ”€ Tune hyperparameters using VALIDATION set
   â””â”€ Can use validation set 100s of times âœ…

3. Model Selection:
   â”œâ”€ Try multiple architectures
   â”œâ”€ Pick best based on VALIDATION performance
   â””â”€ Still haven't touched test set

4. Final Evaluation:
   â”œâ”€ Evaluate on test set ONCE
   â”œâ”€ Report this number
   â””â”€ NEVER go back and modify model!

If you need more rounds:
â””â”€ Create NEW test set (expensive!)
```

---

### ğŸ“ VC Dimension - IMPORTANT FOR INTERVIEWS!

**Definition:**
```
VC Dimension = Maximum number of points that can be 
               perfectly classified with arbitrary labels

For binary classification:
"How many points can we shatter?"
```

**Examples:**

**1. Linear Classifier in 2D:**

```
VC Dimension = 3

Can shatter 3 points:
  â€¢     â€¢     â€¢
   Any labeling â†’ can find a line to separate

Cannot shatter 4 points:
  â€¢ XOR pattern â€¢
     Ã—    Ã—
  Cannot separate with a line!
```

**2. Linear Model in d dimensions:**

```
VC Dimension = d + 1

Example:
â”œâ”€ d=2: VC=3
â”œâ”€ d=10: VC=11
â””â”€ d=784: VC=785
```

---

### ğŸ“Š Generalization Bound

**Theoretical Result:**

```
With probability â‰¥ (1-Î´):

|R - R_emp| â‰¤ Îµ

where Îµ âˆ âˆš(VCÂ·log(n) / n)

In words:
Test error will be close to training error
if we have enough samples
```

**What This Means:**

```
To guarantee Îµ=0.01 with 95% confidence:

n â‰ˆ VCÂ·log(n) / ÎµÂ²

For VC=785 (Fashion-MNIST):
n â‰ˆ 785Â·log(n) / 0.01Â²
n â‰ˆ millions of samples!

But we only use 60,000! ğŸ¤”

Theory is TOO CONSERVATIVE for deep learning
(This is an active research area)
```

---

## ğŸŒ Chapter 4.7: Distribution Shift

### ğŸ¯ The IID Assumption

**What We Assumed Until Now:**

```
Training data ~ P(x, y)
Test data ~ P(x, y)  â† SAME distribution!

This is called IID:
â”œâ”€ Independent
â”œâ”€ Identically
â””â”€ Distributed
```

**When IID Breaks:**

```
Training ~ P_source(x, y)
Test ~ P_target(x, y)  â† DIFFERENT!

Now what? ğŸ˜±
```

---

### ğŸ”€ Types of Distribution Shift

#### **1. Covariate Shift**

```
Definition:
â”œâ”€ P(x) changes  â† Input distribution shifts
â””â”€ P(y|x) same   â† Relationship stays the same

Assumption: x causes y

Example: Cat/Dog Classification

Training Data:
â”œâ”€ Professional photos
â”œâ”€ Good lighting
â”œâ”€ Clear backgrounds
â””â”€ P_train(x) = distribution of pro photos

Test Data:
â”œâ”€ User selfies with pets
â”œâ”€ Poor lighting
â”œâ”€ Cluttered backgrounds
â””â”€ P_test(x) = distribution of amateur photos

But: P(dog | x) is same!
A dog is still a dog regardless of photo quality
```

**Real-World Examples:**

```
1. MEDICAL IMAGING:
   Train: Hospital A's scanner
   Test: Hospital B's scanner
   â†³ Different image quality, same diseases

2. AUTONOMOUS DRIVING:
   Train: California (sunny)
   Test: Seattle (rainy)
   â†³ Different weather, same road rules

3. SPAM DETECTION:
   Train: 2020 emails
   Test: 2025 emails
   â†³ Different writing styles, same spam concept
```

---

#### **2. Label Shift**

```
Definition:
â”œâ”€ P(y) changes  â† Label distribution shifts
â””â”€ P(x|y) same   â† Features given label stay same

Assumption: y causes x

Example: Medical Diagnosis

Training Data (2020):
â”œâ”€ P(flu) = 0.05
â”œâ”€ P(covid) = 0.01
â””â”€ P(healthy) = 0.94

Test Data (2021 - pandemic):
â”œâ”€ P(flu) = 0.02
â”œâ”€ P(covid) = 0.20  â† Big change!
â””â”€ P(healthy) = 0.78

But: P(symptoms | covid) is same!
Covid symptoms don't change
```

**More Examples:**

```
1. SEASONAL PRODUCTS:
   Train: Summer (swimsuits popular)
   Test: Winter (coats popular)
   â†³ Product demand shifts

2. ELECTION PREDICTION:
   Train: Historical elections
   Test: Current election
   â†³ Voter preferences shift

3. CUSTOMER CHURN:
   Train: Pre-pandemic
   Test: Post-pandemic
   â†³ Churn rates changed
```

---

#### **3. Concept Shift**

```
Definition:
â”œâ”€ The MEANING of labels changes
â””â”€ P(y|x) changes fundamentally

Example: Regional Terminology

"Pop" vs "Soda" vs "Coke":
â”œâ”€ Northeast US: "Soda"
â”œâ”€ Midwest US: "Pop"
â”œâ”€ South US: "Coke" (for any soft drink!)
â””â”€ Same product, different names!

Building translation system:
â”œâ”€ Train in Northeast
â”œâ”€ Deploy in South
â””â”€ Completely different concept! âŒ
```

**Other Examples:**

```
1. FASHION:
   2010: "Skinny jeans" = fashionable
   2025: "Skinny jeans" = outdated
   â†³ Concept changed

2. MENTAL HEALTH:
   Diagnostic criteria change over time
   DSM-IV â†’ DSM-V
   â†³ Same symptoms, different diagnosis

3. JOB TITLES:
   "Data Scientist" meant different things
   2010 vs 2025
```

---

### ğŸ› ï¸ Correction Methods

#### **Covariate Shift Correction:**

```
Idea: Reweight training examples

Step 1: Estimate importance weights
Î²áµ¢ = P_target(xáµ¢) / P_source(xáµ¢)

Step 2: Train with weighted loss
L = (1/n) Î£ Î²áµ¢ Â· loss(f(xáµ¢), yáµ¢)
```

**Algorithm:**

```python
# 1. Create binary dataset
# Label=1 if from target, Label=0 if from source
combined = {
    (x from source, label=0),
    (x from target, label=1)
}

# 2. Train binary classifier
h = train_classifier(combined)
P(target | x) = sigmoid(h(x))

# 3. Compute weights
for x in training_data:
    Î² = P(target|x) / P(source|x)
      = sigmoid(h(x)) / (1 - sigmoid(h(x)))
      = exp(h(x))

# 4. Train with weights
for x, y, Î² in weighted_training_data:
    loss = Î² * cross_entropy(model(x), y)
    update_model(loss)
```

---

#### **Label Shift Correction:**

```
Uses: Confusion Matrix

Step 1: Train model on source data

Step 2: Compute confusion matrix C on validation
C[i,j] = P(predict i | true j)

Example (3 classes):
        Trueâ†’ Cat  Dog  Bird
Pred â†“
Cat          0.8  0.1  0.1
Dog          0.1  0.7  0.2
Bird         0.1  0.2  0.7

Step 3: Get predictions on target data
Î¼ = [0.5, 0.3, 0.2]  â† Average predictions

Step 4: Solve for target distribution
CÂ·p = Î¼
p = Câ»Â¹Â·Î¼

Step 5: Compute weights
Î²_class = p_target(class) / p_source(class)
```

---

### âš ï¸ Real-World Failure Cases

#### **1. Medical Diagnostics Disaster:**

```
Goal: Detect disease in older men

Training Data:
â”œâ”€ Sick: Older men (hospital patients)
â””â”€ Healthy: Young students (blood donors)

Model: 99% accuracy! ğŸ‰

Problem:
â”œâ”€ Model learned: Age discrimination
â”œâ”€ Not disease detection!
â””â”€ Failed completely in real world âŒ

Covariate Shift:
â”œâ”€ Age distribution completely different
â”œâ”€ Hormone levels different
â””â”€ Lifestyle factors different
```

#### **2. Self-Driving Car Failure:**

```
Goal: Detect roadside

Training Data:
â”œâ”€ Synthetic images from game engine
â””â”€ All roadsides had SAME texture

Model: Perfect on synthetic test! ğŸ‰

Real World:
â””â”€ Complete failure âŒ

Problem:
Model learned: "That specific texture = roadside"
Not: "Object with this shape/context = roadside"
```

#### **3. Tank Detection (Famous Story):**

```
Goal: Detect tanks in forest

Training Data:
â”œâ”€ Morning photos: No tanks
â””â”€ Noon photos: With tanks

Model: 100% accuracy! ğŸ‰

Problem:
Model learned: Shadows vs no shadows
Not: Tank vs no tank âŒ

Real world: Failed completely
```

---

### ğŸ”„ Types of Learning Problems

#### **1. Batch Learning:**

```
Training Phase:
â”œâ”€ Get all data at once
â”œâ”€ Train model
â””â”€ Deploy model

Deployment:
â”œâ”€ Model is FIXED
â”œâ”€ No more updates
â””â”€ Example: Shipped cat door detector

Pros: Simple, stable
Cons: Can't adapt to changes
```

#### **2. Online Learning:**

```
Continuous Process:
For each time step t:
  1. Observe xâ‚œ
  2. Predict Å·â‚œ = f(xâ‚œ)
  3. Observe true yâ‚œ
  4. Compute loss
  5. Update model
  6. Repeat

Example: Stock Price Prediction
â”œâ”€ Morning: Predict today's price
â”œâ”€ Evening: See actual price
â”œâ”€ Update model
â””â”€ Next day: Repeat

Pros: Adapts to changes
Cons: Complex, can be unstable
```

#### **3. Bandits:**

```
Like online learning but:
â”œâ”€ Finite set of actions (arms)
â”œâ”€ Get reward for chosen action
â””â”€ Don't see rewards for other actions

Example: Ad Selection
For each user:
  1. Choose ad to show (pull arm)
  2. User clicks or doesn't (reward)
  3. Update beliefs about ad value
  4. Repeat

Famous: Multi-Armed Bandit problem
```

#### **4. Reinforcement Learning:**

```
Environment has memory and responds:

Agent â†’ Action â†’ Environment
  â†‘                    â†“
  â†â”€â”€â”€ Reward â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Examples:
â”œâ”€ Chess: Opponent responds to your moves
â”œâ”€ Self-driving: Other cars react
â””â”€ Game playing: Environment changes

More complex than supervised learning!
```

---

### ğŸ¯ Interview Questions on Distribution Shift

**Q1: What is covariate shift?**
```
A: Input distribution P(x) changes but 
   P(y|x) stays same.
   
   Example: Same cat detector, different camera quality
```

**Q2: What is label shift?**
```
A: Label distribution P(y) changes but
   P(x|y) stays same.
   
   Example: Disease prevalence changes, symptoms don't
```

**Q3: How to detect distribution shift?**
```
A: Train binary classifier to distinguish
   source vs target data.
   
   If accuracy >> 50%: Distributions are different
   If accuracy â‰ˆ 50%: Distributions are similar
```

**Q4: How to fix covariate shift?**
```
A: Importance weighting
   Î² = P_target(x) / P_source(x)
   Weight each training example by Î²
```

**Q5: What is VC dimension?**
```
A: Measure of model complexity
   = Max number of points we can shatter
   
   Linear model in d dimensions: VC = d+1
```

---

## ğŸ¯ Technical Terms for Interviews

### Must-Know Definitions

**1. Logits:**
```
Raw network outputs BEFORE softmax
o = Xw + b
Can be any real number (-âˆ to +âˆ)
```

**2. Softmax:**
```
Converts logits to probabilities
Å· = softmax(o)
Output: Valid probability distribution (0-1, sum=1)
```

**3. Cross-Entropy:**
```
Loss function for classification
l = -log(Å·_true_class)
Penalizes wrong predictions
```

**4. One-Hot Encoding:**
```
Represent categories as vectors
q classes â†’ q-dimensional vector
One element = 1, rest = 0
```

**5. Argmax:**
```
Find index of maximum value
preds = argmax(Å·)
Converts probabilities to class prediction
```

**6. Confusion Matrix:**
```
Table showing predictions vs truth
C[i,j] = Count of (predicted=i, true=j)
Diagonal = correct predictions
```

**7. Empirical Risk:**
```
Average loss on training data
R_emp = (1/n) Î£ loss(f(xáµ¢), yáµ¢)
What we can actually minimize
```

**8. Generalization Error:**
```
Expected loss on population
R = E[loss(f(X), Y)]
What we actually care about
```

---

## ğŸ’¡ Key Interview Insights

### Common Questions & Answers

**Q: Why can't we use MSE for classification?**
```
A: Multiple reasons:
   1. Doesn't match probability interpretation
   2. Gradients can be wrong
   3. Not derived from maximum likelihood
   4. Cross-entropy has better theoretical properties
```

**Q: Why softmax and not just normalize?**
```
A: Softmax has key properties:
   1. Exponential â†’ emphasizes differences
   2. Differentiable everywhere
   3. Preserves ordering
   4. Has probabilistic interpretation (Gibbs distribution)
```

**Q: What if two classes have similar probabilities?**
```
A: Model is uncertain!
   Example: [0.49, 0.51]
   
   Solutions:
   â”œâ”€ Collect more training data
   â”œâ”€ Add more features
   â”œâ”€ Use more complex model
   â””â”€ Or: Return top-k predictions with confidence scores
```

**Q: Difference between accuracy and loss?**
```
A: 
LOSS:
â”œâ”€ Continuous, differentiable
â”œâ”€ Used for optimization
â”œâ”€ Measures prediction quality

ACCURACY:
â”œâ”€ Discrete (0 or 1 per example)
â”œâ”€ Not differentiable
â”œâ”€ What users care about
â””â”€ Can't optimize directly
```

**Q: Why flatten images?**
```
A: Linear layers expect 1D input
   
   Image: (28, 28) â†’ 2D
   Flatten: (784,) â†’ 1D
   
   Later: CNNs can handle 2D directly!
```

---

## ğŸ“Š Complete Comparison Table

| Aspect | Linear Regression | Softmax Regression |
|--------|------------------|-------------------|
| **Problem Type** | Regression | Classification |
| **Output** | Single number | Probability distribution |
| **Output Activation** | None (identity) | Softmax |
| **Output Dimension** | 1 | q (num classes) |
| **Loss** | MSE | Cross-Entropy |
| **Label Format** | Continuous value | One-hot vector |
| **Prediction** | Å· directly | argmax(Å·) |
| **Example** | House price | Image category |

---

## ğŸ¯ Algorithm Walkthrough - Complete

### Training Softmax Regression

```python
# SETUP
num_inputs = 784    # 28Ã—28 flattened
num_outputs = 10    # 10 classes
batch_size = 256
lr = 0.1

# INITIALIZE
W = torch.randn(784, 10) * 0.01
b = torch.zeros(10)

# TRAINING LOOP
for epoch in range(max_epochs):
    
    for X_batch, y_batch in train_loader:
        # X_batch: (256, 1, 28, 28)
        # y_batch: (256,)
        
        # 1. FLATTEN
        X = X_batch.reshape(-1, 784)  # (256, 784)
        
        # 2. COMPUTE LOGITS
        o = X @ W + b  # (256, 10)
        
        # 3. SOFTMAX
        y_hat = softmax(o)  # (256, 10)
        
        # 4. LOSS
        loss = cross_entropy(y_hat, y_batch)
        
        # 5. BACKWARD
        optimizer.zero_grad()
        loss.backward()
        
        # 6. UPDATE
        optimizer.step()
        
    # VALIDATION
    with torch.no_grad():
        val_loss, val_acc = evaluate(model, val_loader)
        print(f'Epoch {epoch}: Val Acc = {val_acc:.2%}')
```

---

## ğŸ”¥ Common Mistakes & Fixes

### âŒ Mistake 1: Wrong Loss Function

```python
# WRONG
loss = ((y_hat - y) ** 2).mean()  # MSE for classification âŒ

# RIGHT
loss = F.cross_entropy(logits, y)  # Cross-entropy âœ…
```

### âŒ Mistake 2: Softmax Before Cross-Entropy

```python
# WRONG
probs = F.softmax(logits, dim=1)
loss = F.cross_entropy(probs, y)  # Expects logits! âŒ

# RIGHT
loss = F.cross_entropy(logits, y)  # Give logits directly âœ…
```

### âŒ Mistake 3: Wrong Label Format

```python
# WRONG for PyTorch
y = [[1, 0, 0],      # One-hot âŒ
     [0, 1, 0]]

# RIGHT
y = [0, 1]           # Class indices âœ…
```

### âŒ Mistake 4: Forget to Flatten

```python
# WRONG
X = (64, 1, 28, 28)
o = X @ W  # Shape mismatch! âŒ

# RIGHT
X = X.reshape(64, 784)
o = X @ W  # (64, 784) @ (784, 10) = (64, 10) âœ…
```

---

## ğŸ¯ Final Summary - Must Remember!

### Core Concepts

```
1. SOFTMAX:
   â”œâ”€ Converts logits to probabilities
   â”œâ”€ Formula: exp(oâ±¼) / Î£exp(oâ‚–)
   â””â”€ Properties: Positive, sum to 1

2. CROSS-ENTROPY:
   â”œâ”€ Loss for classification
   â”œâ”€ Formula: -log(Å·_true_class)
   â””â”€ Gradient: Å· - y (simple!)

3. ONE-HOT ENCODING:
   â”œâ”€ Represent categories as vectors
   â””â”€ [0, 0, 1, 0] for class 2

4. DISTRIBUTION SHIFT:
   â”œâ”€ Covariate: P(x) changes
   â”œâ”€ Label: P(y) changes
   â””â”€ Concept: P(y|x) changes

5. GENERALIZATION:
   â”œâ”€ Test on held-out data
   â”œâ”€ Avoid overfitting
   â””â”€ Never tune on test set!
```

### Key Equations

```
Softmax:        Å·â±¼ = exp(oâ±¼) / Î£â‚– exp(oâ‚–)
Cross-Entropy:  l = -Î£â±¼ yâ±¼ log(Å·â±¼) = -log(Å·_true)
Gradient:       âˆ‚l/âˆ‚o = Å· - y
Accuracy:       (# correct) / (# total)
```

 